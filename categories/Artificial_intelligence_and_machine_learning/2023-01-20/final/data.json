{
    "date": "2023-01-20",
    "category": "9D",
    "articles": {
        "0": {
            "text": "As researchers continue to unravel the many mysteries of genomics, they require more and more sophisticated technologies to diagnose, monitor and treat genetic conditions. Artificial intelligence tools, which mimic human intelligence to solve problems, are well-suited to tackle these complex tasks.\n\nMachine learning, a type of artificial intelligence, has the unique ability to learn and improve itself. These clever methods have already been used to predict how a certain type of cancer will progress, find disease-causing genomic variants and identify genetic disorders by examining people\u2019s faces.\n\nResearchers at the National Human Genome Research Institute (NHGRI) are increasingly using artificial intelligence tools to answer compelling questions in genomics, such as predicting rare genetic disorders and their severity, and to understand how genomic information influences decision-making.\n\nMachine learning and other artificial intelligence tools are already improving the detection of relatively common conditions, such as breast cancer through mammography.\n\nBenjamin Solomon, M.D., NHGRI clinical director and senior clinician in the NHGRI Medical Genetics Branch, wants to know if we can find a way to use these tools at surface level\u2014to diagnose genetic conditions that affect the skin.\n\nGenetic disorders are often rare and notoriously difficult to diagnose. On average, it takes between five and 10 years from the onset of symptoms to pinpoint the exact genetic cause of a rare condition. The long and arduous diagnostic journey often delays treatment, and it typically ends up being costly and isolating.\n\nBut that timescale is shrinking. To shorten this process even more, Dr. Solomon and his colleagues are exploring how artificial intelligence tools can help clinicians identify genetic conditions that involve characteristic marks or patterns on the skin, similar to birthmarks.\n\n\u201cIf we can create a consistent system to recognize genetic skin conditions across the board, we can help clinicians diagnose disorders right at the bedside. The sooner we can make an accurate diagnosis, the faster we can treat the patient.\u201d\n\nIn a recent study, they used computing systems to recognize and classify rare genetic skin disorders based on photographs that illustrated different abnormal skin characteristics. The underlying algorithm, which is a set of programmed instructions for solving tasks, could classify genetic skin conditions more accurately than pediatricians or medical geneticists.\n\nThese computing systems, called neural networks, mimic how the brain processes information. Researchers must train the neural networks to associate specific skin characteristics with the corresponding genetic conditions, much in the same way a person would show photographs of apples and oranges to teach children the difference between these fruits.\n\nSo far, the study has taught the algorithm to recognize six genetic skin conditions, and Dr. Solomon hopes to expand this tool to diagnose more rare genetic disorders.\n\nMARKING THE SEVERITY OF RARE DISORDERS\n\nOleg Shchelochkov, M.D., NHGRI director of residency and fellowship programs, is also harnessing the power of artificial intelligence to help diagnose rare genetic disorders more accurately.\n\nSpecifically, Dr. Shchelochkov is interested in a rare metabolic disorder called propionic acidemia, which affects one in 20,000 to 500,000 people worldwide. Patients with propionic acidemia have higher levels of a chemical called propionic acid in their bodies, which can cause organ damage and frequent hospitalizations. In some cases, a liver transplant is necessary.\n\nFor decades, researchers and clinicians have discussed the possibility of two types of propionic acidemia\u2014mild and severe\u2014which could have an impact on the type of treatment that a patient receives. But because of the limited number of people with this condition, researchers have found it difficult to predict which patients might benefit from the different treatment approaches.\n\nRecently, Dr. Shchelochkov published a study with Charles Venditti, M.D., Ph.D., chief of the NHGRI Metabolic Medicine Branch, that used machine learning to find biological markers, also called biomarkers, associated with mild and severe forms of the condition.\n\nThe researchers collected nearly 500 types of genetic, laboratory and imaging data. After working with propionic acidemia disease experts to create a system to classify patients into mild and severe categories, the researchers trained the algorithm to determine which pieces of the data are uniquely associated with the two forms of the disease. After training, the researchers gave the algorithm new patient information. The algorithm was very successful at establishing which data types were associated with the mild versus severe form of propionic acidemia.\n\n\u201cIf we can use machine learning to make these kinds of useful predictions about rare diseases, even with such little data, it would be a boon for more common conditions like cancer, hypertension and diabetes.\u201d\n\nThe results of this study support a decades long intuition held by experienced clinicians that there are distinct versions of propionic acidemia. With early insights into the severity of a given case, clinicians can better design the treatment plan for that patient.\n\nWith information about which biomarkers are most closely associated with the severity of propionic acidemia, clinicians can focus on identifying severe patients more rapidly and provide them with the help they need as early as possible.\n\n\u201cIf we can use machine learning to make these kinds of useful predictions about rare diseases, even with such little data, it would be a boon for more common conditions like cancer, hypertension and diabetes,\u201d says Dr. Venditti.\n\nInstead of using artificial intelligence to understand and recognize rare genetic disorders, Susan Persky, Ph.D., associate investigator in the NHGRI Social and Behavioral Research Branch, is using such tools to understand how different ways of communicating genomic information can influence behavior.\n\nDr. Persky heads the Immersive Simulation Program at NHGRI, in which her research group uses virtual reality to simulate realistic interactions to assess how delivering individual genomic information can affect someone\u2019s decisions and behavior.\n\nBecause both genomics and environmental factors, such as food choices, influence health, these researchers are interested in studying how parents choose food for their children after learning about their child\u2019s genomic risk for health conditions.\n\n\u201cUsing immersive virtual reality technology and machine learning analysis, we want to develop ways for researchers and clinicians to improve how we communicate genomic risk to individuals.\u201d\n\nTo examine this dynamic, the researchers created a simulated food buffet where parents can select lunch for their child, just like they would in real life.\n\n\u201cWe want to know what ends up on the plate, but that\u2019s just step one,\u201d says Dr. Persky. \u201cThanks to this virtual reality approach, we can collect a lot of information about the parents\u2019 behavior, like where the parents are walking, what they are looking at, the order in which their choices are made and how long they took at each step.\u201d",
            "title": "Researchers explain how they use AI in genomic studies - Hometown Focus",
            "keywords": [
                "intelligence",
                "studies",
                "focus",
                "nhgri",
                "conditions",
                "genetic",
                "information",
                "hometown",
                "rare",
                "propionic",
                "researchers",
                "skin",
                "genomic",
                "ai",
                "learning",
                "explain"
            ],
            "link": "https://www.hometownfocus.us/articles/researchers-explain-how-they-use-ai-in-genomic-studies/",
            "skip": "true",
            "gpt_score_reason": "\n\nThe researchers use machine learning to analyze the data collected and examine how different ways of communicating genomic information can influence a parents\u2019 decisions. With this knowledge, they hope to eventually develop better ways for clinicians to communicate genetic information effectively in clinical settings.\n\n8/10",
            "gpt_relevancy_score": 0
        },
        "1": {
            "text": "Douglas, Jan. 20, 2023 (GLOBE NEWSWIRE) -- Between 2022 and 2030, the global machine learning in the life sciences market is expected to grow rapidly. In fact, as a whole, the market for machine learning behaved very differently from a lot of other markets over the past few years - particularly during the COVID-19 period. Where other industries faced declines and stagnation, there was a staggering demand for machine learning across all regions, and had much greater growth between 2020 and 2019.\n\nWhen it comes to life sciences, the industry is still at its base level. There is a great amount of potential and improvement still possible, and plenty more to go in the long-run. Machine learning and artificial intelligence is proving to be a tipping point for growth in the life sciences market. Not only does it allow for more accurate predictions and a better understanding of the life sciences, but in many cases becomes a crucial element to saving lives - particularly in the medical and drug discovery sector.\n\nThe different players in the global machine learning in the life sciences market are investing in adopting AI and ML based technologies at a rapid rate, since the competitiveness of the market now demands the use of these two technologies. There is also an increasing amount of applications for AI - and thus, ML - in analyzing datasets that enhance drug discovery and develop personalized medicines for specific problems.\n\nBrowse to access an In-depth research report on Global Machine Learning in the Life Sciences Market with detailed charts and figures: https://douglasinsights.com/life-sciences-machine-learning-market\n\nCurrently, the largest share of the global machine learning in life sciences market is in North America, while the fastest growing region is Europe.\n\nThe major segments in the market are:\n\nOffering\n\nDeployment\n\nApplication\n\nGeography\n\nIn terms of offering, the software segment dominated the market in 2021, because of the growing need for storage, management, analysis and sharing of important data in drug discovery and clinical trials. With the rise of big data and analytics, the demand for AI and ML software is also growing. Software is also a recurring revenue stream, and thus the major revenue contributor towards global machine learning in life sciences market.\n\nDue to lack of understanding around how machine learning works, life sciences professionals also need third party service providers who can run the software itself to get meaningful insights and analysis out of it.\n\nIn terms of deployment, the growing use of the internet, cloud-based services and servers are also leading to the growth of deployment of machine learning models via cloud services. This segment has a high growth rate, and is forecasted to keep growing until 2030.\n\nIn 2021, the application of machine learning models was greatest in drug discovery and development. Since chronic and genetic diseases are prevalent and growing, the investments towards drug discovery are also growing, and thus machine learning models are being used widely in pharma and biotech companies for new and innovative drugs.\n\nHowever, clinical trials are considered to be the most opportunistic in terms of application, since drug discovery growth resulted in a greater number of clinical trials as the drugs were developed, and this is expected to keep growing over the forecast period.\n\nTo provide a clearer idea on the current state of the global machine learning in the life sciences market, the Douglas Insights report contains a full analysis. The report goes over the current market potential, as well as projections for the future, and provides a detailed analysis of the competitive environment, what regulations are in place as well as key drivers. It also looks at the restraints, opportunities and trends within the global machine learning in the life sciences market.\n\nThe report considers the projections for the market from 2022 to 2027 and takes a detailed look at ML technologies as they are used in the life sciences sector. It identifies the key players in the market and the current status of the market itself, as well as providing forecasts for growth over the next five years.\n\nThe report highlights the major challenges and advances from a scientific perspective as well as the latest trends. Being a sensitive field, there are a number of government regulations in place and these are also addressed.\n\nBrowse to access an In-depth research report on Global Machine Learning in the Life Sciences Market with detailed charts and figures: https://douglasinsights.com/life-sciences-machine-learning-market\n\nThe industry is also frequently affected by a number of different factors worldwide, which are taken into account in the analysis, as well as the recent patents and collaborations between players.\n\nMachine learning in life sciences technologies are constantly growing as new discoveries and algorithms come to the surface, affecting the present and future market status and any growth forecasts. By taking a look at these factors, it is easier for organizations involved in the industry to make smart decisions at the right time to gain a competitive advantage.\n\nThe report includes 32 data tables, and 28 additional ones. It looks at all the factors affecting the market, as highlighted above, and also identifies the major stakeholders and competitors within the industry, and how they affect development and segmental revenues.\n\nThere is also an emphasis on the growth strategies that are adopted by the major players within the global market, such as Alteryx Inc, KNIME AG, Microsoft Corp and others. This also affects the market share and growth, and these factors are taken into account.\n\nWith the rapid growth in ML technologies, the Douglas Insights report on the global machine learning in the life sciences market is a necessity for any organization in the industry to make good business decisions and improve performance.\n\nSet a budget for a custom project and see offers from publishers all over the world: https://douglasinsights.com/projects\n\nCountries Covered in the report are as below:\n\nNorth America - United States, Canada, and Mexico\n\nEurope - United Kingdom, Germany, France, Italy, Russia, Spain & Rest of Europe\n\nAsia-Pacific - China, India, Japan, South Korea, Australia & Rest of APAC\n\nLatin America - Brazil, Argentina, Peru, Chile & Rest of Latin America\n\nMiddle East and Africa - Saudi Arabia, UAE, Israel, South Africa\n\nSegmentation Covered into Global Machine Learning in the Life Sciences Market-\n\nMarket Breakdown by Offering\n\n6.1 Software\n\n6.1.1 Market Size and Forecast\n\n6.2 Services\n\n6.2.1 Market Size and Forecast\n\nMarket Breakdown by Deployment Mode\n\n7.1 Cloud\n\n7.1.1 Market Size and Forecast\n\n7.2 On-premises\n\n7.2.1 Market Size and Forecast\n\nMarket Breakdown by Application\n\n8.1 Diagnosis\n\n8.1.1 Market Size and Forecast\n\n8.2 Therapy\n\n8.2.1 Market Size and Forecast\n\n8.3 Healthcare Management\n\n8.3.1 Market Size and Forecast\n\nKey questions answered in this report\n\nCOVID 19 impact analysis on global Machine Learning in the Life Sciences industry.\n\nWhat are the current market trends and dynamics in the Machine Learning in the Life Sciences market and valuable opportunities for emerging players?\n\nWhat is driving Machine Learning in the Life Sciences market?\n\nWhat are the key challenges to market growth?\n\nWhich segment accounts for the fastest CAGR during the forecast period?\n\nWhich product type segment holds a larger market share and why?\n\nAre low and middle-income economies investing in the Machine Learning in the Life Sciences market?\n\nKey growth pockets on the basis of regions, types, applications, and end-users\n\nWhat is the market trend and dynamics in emerging markets such as Asia Pacific, Latin America, and Middle East & Africa?\n\nUnique data points of this report\n\nStatistics on Machine Learning in the Life Sciences and spending worldwide\n\nRecent trends across different regions in terms of adoption of Machine Learning in the Life Sciences across industries\n\nNotable developments going on in the industry\n\nAttractive investment proposition for segments as well as geography\n\nComparative scenario for all the segments for years 2018 (actual) and 2028 (forecast)\n\nAccess complete report- https://douglasinsights.com/life-sciences-machine-learning-market\n\nInquire (for customization, for specific regions, etc.): https://douglasinsights.com/static/contact-us\n\nFollow Douglas Insights for More Industry Updates- @ LinkedIn & Twitter\n\nAbout Douglas Insights-\n\nDouglas Insights UK limited is the first company to provide comparison of market research reports by Table of content, price, ratings and number of pages. We understand the value of time. Productivity and efficiency are possible when you take prompt and assured decisions. With our advanced algorithm, filters, and comparison engine, you can compare your preferred reports simultaneously, based on publisher rating, published date, price, and list of tables. Our data portal enables you to find and review the reports from several publishers. You can evaluate numerous reports on the same screen and select the sample for your best match.\n\n\n\n",
            "title": "Revolutionizing the Life Sciences Industry: Machine Learning Market Analysis and Opportunities, 2018-2027 | Douglas Insights",
            "keywords": [
                "size",
                "market",
                "global",
                "douglas",
                "machine",
                "report",
                "industry",
                "growth",
                "opportunities",
                "learning",
                "life",
                "revolutionizing",
                "growing",
                "insights",
                "sciences"
            ],
            "link": "https://www.globenewswire.com/news-release/2023/01/20/2592251/0/en/Revolutionizing-the-Life-Sciences-Industry-Machine-Learning-Market-Analysis-and-Opportunities-2018-2027-Douglas-Insights.html",
            "skip": "false",
            "gpt_score_reason": "\n\n8/10",
            "gpt_relevancy_score": 8,
            "summary": "\n\nThe global machine learning in the life sciences market is seeing rapid growth due to a surge in demand for more accurate predictions and better understanding of life sciences. Investment into using AI and ML technologies is on the rise, with software being the biggest revenue contributor. The market's largest share is currently North America while Europe has seen fastest growth over the past couple years, especially during COVID-19. Currently, drug discovery and development are one of main applications of ML models along with clinical trials that offer many opportunities for further development. Douglas Insights provides an in depth research report on this topic including analysis of factors affecting market potential, current competitive environment, regulations & trends as well as future projections until 2027."
        },
        "2": {
            "text": "\n\n\n\nQuantum machine learning is a rapidly growing field that is revolutionizing artificial intelligence and machine learning. With the help of quantum computing, we can tackle problems that are too complex for classical computers to solve. From advanced robotics to artificial intelligence, quantum machine learning is at the forefront of the latest technological advances. In this blog, we'll explore what quantum machine learning is, the benefits it offers, the algorithms used, and its various applications.\n\nWhat is quantum machine learning?\n\nQuantum machine learning is an emerging field that combines the power of quantum computing and machine learning. By leveraging the advantages of quantum computing, quantum machine learning can solve complex problems that are impossible for classical computers. This approach uses quantum algorithms to process and analyze data, making it possible to achieve breakthroughs in artificial intelligence and machine learning.\n\nAt its core, quantum machine learning is all about using the power of quantum computing to solve complex problems that are too difficult for classical computers. It makes use of quantum algorithms to process and analyze data, allowing us to solve problems that would otherwise be impossible to solve with traditional computing methods.\n\nOne of the main benefits of quantum machine learning is its increased speed and accuracy. By using quantum algorithms, we can process data much faster than traditional methods. This makes it possible to solve complex problems in a much shorter amount of time, offering improved accuracy and performance. Additionally, quantum machine learning is more energy efficient as it requires less energy to process data.\n\nAnother advantage of quantum machine learning is its ability to handle complex data sets. Using quantum algorithms, we can process large amounts of data in a shorter amount of time. This means that quantum machine learning can be used to analyze and interpret complex data sets, making it ideal for AI and machine learning applications.\n\nBenefits of quantum machine learning\n\nQuantum machine learning offers a number of benefits to both businesses and individuals. For businesses, quantum machine learning can offer improved efficiency and accuracy, allowing them to solve complex problems in a shorter amount of time. Additionally, quantum machine learning is more energy-efficient than traditional methods, saving businesses money in the long run.\n\nFor individuals, quantum machine learning can open up new opportunities. With the help of quantum algorithms, we can process and analyze data much faster than with traditional methods. This makes it possible to solve complex problems and develop new applications in a much shorter amount of time. Additionally, quantum machine learning can help individuals understand complex data sets, making it easier to spot patterns and make decisions.\n\nQuantum machine learning algorithms\n\n\n\nGrover\u2019s algorithm: Grover\u2019s algorithm is used for searching and sorting data. It uses quantum computing to speed up the searching and sorting of large data sets.\n\nQuantum annealing: Quantum annealing is used to optimize complex problems. It makes use of quantum computing to find the best solution to a given problem.\n\nAdiabatic quantum optimization: Adiabatic quantum optimization is used to find the global optimum of a given problem. It makes use of quantum computing to find the best solution.\n\nVariational quantum algorithms: Variational quantum algorithms are used to solve optimization problems. They make use of quantum computing to optimize the parameters of a given problem.\n\nQuantum reinforcement learning: Quantum reinforcement learning is used to solve problems in a dynamic environment. It uses quantum computing to learn from its environment and adapt to changing conditions. There are several quantum algorithms that are used in quantum machine learning. These algorithms rely on the principles of quantum mechanics to process and analyze data. The most commonly used algorithms include the following: These algorithms are used to process and analyze data, allowing us to achieve breakthroughs in artificial intelligence and machine learning.\n\nApplications of quantum machine learning\n\n\n\nRobotics: Quantum machine learning can be used to develop advanced robots that are capable of performing complex tasks. It can be used to optimize robotics algorithms and develop robots that can interact with their environment.\n\nRobotics: Quantum machine learning can be used to develop advanced robots that are capable of performing complex tasks. It can be used to optimize robotics algorithms and develop robots that can interact with their environment. Artificial intelligence: Quantum machine learning can be used to develop more advanced artificial intelligence applications. It can be used to analyze large amounts of data and make decisions based on the data.\n\nOptimization: Quantum machine learning can be used to optimize complex problems. It can be used to find the best solution to a given problem in a much shorter amount of time.\n\nDrug discovery: Quantum machine learning can be used to identify new drugs. It can be used to analyze large amounts of data and identify new molecules that can be used to treat diseases.\n\nImage recognition: Quantum machine learning can be used to identify and classify objects in images. It can be used to detect and classify objects in real time with greater accuracy than traditional methods. Quantum machine learning has a wide range of applications. It can be used to develop advanced robotics, improve artificial intelligence, and optimize complex problems. Additionally, quantum machine learning can be used to develop new applications and understand complex data sets. Here are some of the most common applications of quantum machine learning:\n\nThese are just a few of the applications of quantum machine learning. As more companies adopt quantum computing, we will see even more applications of quantum machine learning in the future.\n\nConclusion\n\nQuantum machine learning is a rapidly growing field that is revolutionizing artificial intelligence and machine learning. By leveraging the power of quantum computing, we can solve complex problems that are too difficult for classical computers. From advanced robotics to artificial intelligence, quantum machine learning is at the forefront of the latest technological advances. As more companies adopt quantum computing, we will see even more applications of quantum machine learning in the future.\n\nIf you're looking to unlock the future of AI, quantum machine learning is the way to go. With the help of quantum algorithms, we can process and analyze data much faster than with traditional methods. This makes it possible to solve complex problems and develop new applications in a much shorter amount of time. So, if you're looking to stay ahead of the curve, quantum machine learning is the way to go!",
            "title": "Unlock the Future of AI with Quantum Machine Learning!",
            "keywords": [
                "problems",
                "quantum",
                "algorithms",
                "computing",
                "machine",
                "unlock",
                "solve",
                "used",
                "data",
                "learning",
                "ai",
                "complex",
                "future"
            ],
            "link": "https://www.thecrydsdaily.com/2023/01/unlock-future-of-ai-with-quantum.html",
            "skip": "false",
            "gpt_score_reason": "\n\n9/10",
            "gpt_relevancy_score": 9,
            "summary": "\n\nQuantum machine learning is a rapidly growing field that combines the power of quantum computing and machine learning. It enables us to solve complex problems that are too difficult for classical computers and can be used for improved efficiency, accuracy, energy-efficiency, speed and applications in AI and robotics. Quantum algorithms allow us to process data much faster than traditional methods, meaning it can be used to analyze large amounts of data sets quickly. With more companies leveraging quantum computing technology, we'll see increasingly advanced uses of quantum machine learning in the future."
        },
        "3": {
            "text": "Reddit Vote Share 0 Shares\n\nA recent study from the NYU Grossman School of Medicine and Meta AI Research shows that artificial intelligence (AI) can rebuild coarse-sampled, fast MRI scans into high-quality pictures with equal diagnostic value as those created using standard MRI. According to the study, MRI images may be made available to more patients, and appointment wait times might be cut in half if they were reconstructed using AI rather than traditional methods. Researchers from Meta AI and NYU Langone\u2019s imaging experts and radiologists collaborated on an AI model to speed up MRI. It also generated the world\u2019s biggest raw MRI data repository, which researchers and developers have utilized in various fields.\n\nThe NYU Langone scientists eliminated nearly three-quarters of the raw data gathered by traditional, sluggish MRI scans to replicate expedited scans in a previous \u201cproof-of-principle\u201d study. Faster MRI scans were used to train an artificial intelligence model that produced pictures that were otherwise indistinguishable from those produced by slower scans. Similar to how the brain constructs pictures by filling in missing visual information from the local context and past experiences, the researchers in this new study performed expedited scans with just one-fourth of the real data and used the AI model to \u201cfill in\u201d the missing information. The fastMRI scans were accurate and of higher quality than the conventional scans in both experiments.\n\nWe used 298 clinical 3-T knee assessment pictures to train a DL reconstruction model. Between January 2020 and February 2021, patients who were clinically referred for knee MRI completed a conventional accelerated knee MRI protocol at 3 T followed by an accelerated DL procedure for a prospective study. It was determined whether or not the DL reconstructed pictures were interchangeable with the traditional images in anomaly identification. Six musculoskeletal radiologists looked at each exam. Ordinal ratings from 0 to 4 were used in analyses evaluating the probability of anomalies in meniscal or ligament tears, bone marrow, and cartilage. Overall image quality, presence of artifacts, sharpness, and signal-to-noise ratio were all evaluated, and four-point ordinal values were used to compare the methods.\n\n170 people were assessed (mean age SD: 45 16; 76 male). It was found that the DL-reconstructed photos were just as good as the traditional images in spotting anomalies. On average, DL photos received a higher quality rating among six readers than traditional photographs (P .001). The radiologists agreed that the AI-reconstructed images were just as good as the traditional images for making diagnoses of tears and abnormalities. They also decided that the faster scans had far higher image quality overall.\n\nResearchers stress that no unique tools are needed to perform FastMRI. Standard MRI machines may be programmed to collect fewer data than is often required. The fastMRI effort has released its data, models, and code as an open-source project for use by other researchers and makers of commercial MRI systems.\n\nWith fastMRI, the time it takes to perform an MRI scan, which may take up to 30 minutes, is reduced to less than 5 minutes, bringing it on par with the time it takes to perform an X-ray or CT scan. In contrast to these other imaging modalities, however, MRI offers a wealth of information, such as seeing soft tissues from numerous angles, highlighting microscopic cartilage anomalies, and pinpointing abdominal malignancies.\n\nIn conclusion, deep learning reconstruction of prospectively accelerated knee MRI allowed for a near-twofold decrease in scan time, enhanced picture quality, and had equal diagnostic usefulness compared to traditional reconstruction. It has been shown that deep learning reconstruction may cut scan time for a knee MRI by about half compared to the standard procedure without sacrificing diagnostic accuracy.\n\nCheck out the Paper and Reference Article. All Credit For This Research Goes To the Researchers on This Project. Also, don\u2019t forget to join our Reddit Page, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.",
            "title": "This Artificial Intelligence (AI) Research From NYU and Meta AI Proposes A Deep Learning Technique That Can Reconstruct Missing Data From Rapid MRI Scans",
            "keywords": [
                "knee",
                "reconstruct",
                "mri",
                "study",
                "rapid",
                "quality",
                "technique",
                "missing",
                "proposes",
                "nyu",
                "researchers",
                "traditional",
                "scans",
                "research",
                "ai",
                "pictures",
                "used"
            ],
            "link": "https://www.marktechpost.com/2023/01/19/this-artificial-intelligence-ai-research-from-nyu-and-meta-ai-proposes-a-deep-learning-technique-that-can-reconstruct-missing-data-from-rapid-mri-scans/",
            "skip": "true",
            "gpt_relevancy_score": 0
        },
        "4": {
            "text": "Artificial intelligence (AI) and machine learning (ML) are becoming an essential component of modern business strategy. More enterprises across almost all sectors and industries are transforming their businesses with AI/ML. The global AI market value is estimated to reach $267 billion by 2027, with the technology contributing $15.7 trillion to the global economy by 2030. Below are the top 3 take away from our customer's use case in this e-book \u2018Keys to Successful Innovation through Artificial Intelligence\u2019.\n\nBoosting productivity by making prompt and better decisions to enhance customer experiences and satisfaction\n\nPrior to the pandemic, the BMW Group has been efficiently deploying AI/ML, but the pandemic across the globe has created a heightened sense of urgency to take all its AI capabilities remotely. \u201cA comprehensive application of AI/ML and allows us to gain insight from all the data we had,\u201d Josef Viehhauser, platform lead and enterprise analytics at the BMW Group explains.\n\n\"Collaborating with a strong implementation partner is also vital as it may broaden your understanding of the possible, and create a quicker return on investment,\u201d says Mark Maenner, BMW Group's head of data transformation.\n\nAdopting AI/ML is never the goal; AI exists to support the overall business objective developed by considering the larger picture goal. Maenner further points out that AI and ML play an integral role in the digital transformation of the BMW Group and help the Group improve their product experience for customers, the way they develop their products, or even understanding processes.\n\nRethinking silos and fragmented legacy systems, adding capabilities\n\nAI/ML are immensely valuable not only in the healthcare industry, but also paves the way to time and productivity savings for human resources management software and services. The HR technology company, according to Jack Berkowitz, chief data officer at ADP Inc., had already moved its people analytics and workforce benchmarks to the cloud, with the pandemic as a force multiplier for its use of AI/ML.\n\nBy applying AI/ML, all relevant information clients needed for their Paycheck Protection Program applications was available and accessible in the cloud within 20 days. \u201cWe were able to build new capabilities that we never thought possible,\u201d Berkowitz remarks.\n\n\u201cWe already had use cases running on our supply chains, most of which are quite simple events,\u201d he continues. \u201cThe pandemic brought an understanding of the influence of catastrophes on the supply chain in specific regions. We were also able to boost our existing learnings to move us forward under the influence of catastrophes on the supply chain in specific regions and get better in those areas.\u201d\n\nImplementing a successful strategy for AI/ML vary depending on each business\n\nAccording to Kirk Borne, Chief Science Officer at Leesburg, the chances of a successful AI/ML-powered strategy greatly improve if business leaders remember that data is a science, and that its implementation requires an interactive process of testing, validating, and refining the hypothesis, followed by testing and refining the next iteration.\n\nDiscover more important elements to successfully innovate through Artificial Intelligence here.",
            "title": "Explore the crucial elements for adopting and implementing Artificial intelligence (AI) and machine learning (ML)",
            "keywords": [
                "implementing",
                "explore",
                "machine",
                "crucial",
                "successful",
                "strategy",
                "data",
                "ai",
                "group",
                "understanding",
                "intelligence",
                "pandemic",
                "supply",
                "elements",
                "ml",
                "aiml",
                "artificial",
                "bmw",
                "learning"
            ],
            "link": "https://www.thestandard.com.hk/breaking-news/section/4/199430/Explore-the-crucial-elements-for-adopting-and-implementing-Artificial-intelligence-(AI)-and-machine-learning-(ML)",
            "skip": "false",
            "gpt_score_reason": "\n\n8/10",
            "gpt_relevancy_score": 8,
            "summary": "\n\nAI and ML are becoming an essential component of modern business strategy, with the global AI market value estimated to hit $267 billion by 2027. Use cases from BMW Group and ADP Inc. show how businesses can identify objectives, collaborate with partners, rethink silos and fragmented legacy systems, add capabilities and apply a successful AI/ML strategy to better inform their decision making process. Discover more insights here in this e-book on successfully innovating through Artificial Intelligence."
        },
        "5": {
            "text": "In research published in Nature Medicine today , AI biotech company Owkin has demonstrated for the first time that federated learning (FL) can be used to train deep learning models on data from multiple hospitals on histopathology data without the data leaving hospital firewalls.\n\nThe discovery paves the way for AI-powered medical research using larger multi-centric datasets, allowing models to escape the biases of single-centric studies. This has the potential to unlock breakthroughs in precision medicine through the use of secure and privacy-preserving AI.\n\nUsing data kept within four leading French hospitals, Owkin built AI models that can accurately predict the future response of triple-negative breast cancer (TNBC) patients to neoadjuvant chemotherapy. By using interpretable AI to extract information from digital pathology slides, Owkin was able to find potential novel biomarkers. This could in the future help funnel patients towards either less toxic treatments or new experimental treatments, improving the personalization of medical care.\n\nThe project used federated learning \u2013 a collaborative AI framework that preserves data privacy and security through Substra, Owkin\u2019s recently-open sourced software that make every operation traceable using hyperledger technology. The research is the first time that machine learning models have been trained using histopathology data from multiple hospitals without the data leaving the hospitals. Previously, most studies were restricted to simulating FL by artificially splitting data. It is a landmark proofpoint of FL in medical research and represents a breakthrough in the realization of AI\u2019s practical benefit to research.\n\nThe study used digital pathology data and clinical information from 650 patients from Institut Curie in Paris, Centre L\u00e9on B\u00e9rard in Lyon, Gustave Roussy in Villejuif and IUCT Oncopole in Toulouse, making it one of the largest TNBC cohorts of its kind ever assembled for such kind of analysis.\n\nThe research builds on Owkin\u2019s pioneering use of FL to enable pharmaceutical companies to collaborate on drug discovery research while safeguarding privacy, security and competitive considerations. The MELLODDY project\u2019s results , published this year, showed that collaborating in AI for drug discovery is possible at industrial scale thanks to FL, a first for the industry. In addition to addressing privacy and security concerns, FL can also simplify data governance issues, removing the need to transfer data and fostering more collaborative research.\n\nJean du Terrail, lead author and Senior Machine Learning Scientist at Owkin, said:\n\nThanks to our partners, we are proud to have performed an original federated analysis on medical data in real-life conditions, and the first of its kind on histopathology data. By connecting institutions in a federated manner we were able to reach the critical mass of triple negative breast cancer data necessary for the AI to discover, on its own, histological patterns predictive of the response to treatment. We hope that this proof of concept will inspire medical institutions to collaborate in federated learning networks in order to move research forward while keeping patient data private.\n\nJulien Gu\u00e9rin, Chief Data Officer at Institut Curie in Paris, France, said:\n\nWe reached an important milestone with the deployment of this federated learning infrastructure, showing a new cutting-edge approach of building AI in cancer research. We are really happy to have been part of this adventure and hope this will open promising perspectives for the future of patient care.\n\nDr Guillaume Bataillon, pathologist at IUCT Oncopole in Toulouse, France, and former pathologist at Institut Curie in Paris, France, said:\n\nThrough this multidisciplinary partnership, we verify the feasibility of an inter hospital collaborative federated learning approach on a relevant biological question. This allowed us to create pooled heterogeneous dataset in a secure and faster way in order to develop reproductible, transferable and even interpretable models. This proof of concept has the potential to become a tool for therapeutic decision.\n\nDr Pierre Etienne Heudel, Medical oncologist at Centre L\u00e9on B\u00e9rard in Lyon, France, said:\n\nThe rise of digital pathology coupled with the explosion of different machine learning techniques should enable increasingly precise and personalized medicine. Moreover, the federated learning, achieved in this project by avoiding external data streams, facilitates and secures the process for future daily clinical practice.\n\nDr Magali Lacroix-Triki, pathologist at Gustave Roussy in Villejuif, France, said:\n\nDigital pathology and AI represent the third revolution in the world of pathology, and pathologists are excited to lead this new change in their practice. Federated learning, pioneering AI research in digital pathology, brings us one step closer to identifying new biomarkers in oncology while ensuring data privacy and security.\n\nDr Camille Franchet, pathologist at IUCT Oncopole in Toulouse, France, said:\n\nBy enabling AI models to be trained on multicentric data without centralization, federated learning unlocks one of the major obstacles in machine learning on medical data with no compromise regarding the respect of personal data.\n\nAbout Owkin\n\nOwkin is an AI biotechnology company that uses artificial intelligence to find the right treatment for every patient. We bridge shared innovation challenges between biopharma and academic researchers and close the translational gap between complex biology and new drugs.\n\nWe use AI to identify new treatments, de-risk and accelerate clinical trials and build diagnostic tools that improve patient outcomes. Using federated learning, a pioneering collaborative AI framework, Owkin enables medical and biopharma partners to unlock valuable insights from siloed datasets while protecting patient privacy and securing proprietary data.\n\nOwkin was co-founded by Thomas Clozel MD, a former assistant professor in clinical onco-haematology, and Gilles Wainrib, a pioneer in the field of machine learning in biology, in 2016. Owkin has raised over $300 million and became a unicorn through investments from leading biopharma (Sanofi and BMS) and venture funds (Fidelity, GV and BPI, among others).",
            "title": "Nature Medicine publishes breakthrough Owkin research on the first ever use of federated learning to train deep learning models on multiple hospitals\u2019 histopathology data",
            "keywords": [
                "medicine",
                "france",
                "ai",
                "publishes",
                "multiple",
                "hospitals",
                "nature",
                "medical",
                "models",
                "pathology",
                "data",
                "federated",
                "learning",
                "research",
                "owkin",
                "train"
            ],
            "link": "https://www.eurekalert.org/news-releases/976986",
            "skip": "false",
            "gpt_score_reason": " 8/10",
            "gpt_relevancy_score": 8,
            "summary": "\n\nOwkin has demonstrated, for the first time, that federated learning can be used to train deep learning models on data from multiple hospitals without having to leave the hospital firewalls. This is a breakthrough in precision medicine as it allows AI-powered medical research using larger datasets while ensuring privacy and security through Substra's open sourced software. This could facilitate novel biomarkers discovery which could improve personalization of medical care. 650 patients were included in this study - one of the largest cohorts ever assembled for such kind of analysis - proving FL as an effective tool for collaboration and data governance across institutions."
        },
        "6": {
            "text": "INVOX Medical is a medical transcription solution that helps enables physicians to report and enter clinical information into the EHR through dictation, saving time and streamlining their workflow. In addition, dictated reports are richer in detail and the use of abbreviations is reduced. With INVOX Medical, transcription is done automatically and in real-time, wherever the doctor has placed the cursor, and the report is ready as soon as the dictation is finished, avoiding double validations and administrative staff having to transcribe the audio. These dictionaries have been created by processing tens of thousands of anonymized medical reports using machine learning and artificial intelligence techniques. We also have a support and service team that guarantees a res...\n\nRead more",
            "title": "INVOX Medical Software Reviews, Demo & Pricing",
            "keywords": [
                "reviews",
                "pricing",
                "software",
                "demo",
                "reports",
                "report",
                "medical",
                "dictation",
                "workflow",
                "transcribe",
                "using",
                "invox",
                "validations",
                "transcription"
            ],
            "link": "https://www.softwareadvice.com/medical/invox-medical-profile/",
            "skip": "true",
            "gpt_score_reason": " \n3/10",
            "gpt_relevancy_score": 3
        },
        "7": {
            "text": "Here's how you know\n\nAn official website of the United States government",
            "title": "Artificial Intelligence Summit at NPS Accelerates Critical Capabilities",
            "keywords": [
                "intelligence",
                "united",
                "capabilities",
                "summit",
                "critical",
                "nps",
                "states",
                "knowan",
                "artificial",
                "official",
                "heres",
                "accelerates",
                "website"
            ],
            "link": "https://www.navy.mil/Press-Office/News-Stories/Article/3272831/artificial-intelligence-summit-at-nps-accelerates-critical-capabilities/",
            "skip": "true",
            "duplicates": [
                "7",
                "19"
            ],
            "gpt_score_reason": " recently revealed a new initiative to increase the public's understanding of artificial intelligence and machine learning.7/10",
            "gpt_relevancy_score": 7
        },
        "8": {
            "text": "Farmington, Jan. 19, 2023 (GLOBE NEWSWIRE) -- The Global Artificial Intelligence (AI) In Retail Market Size Accounted For USD 8.41 Billion In 2022 And It Is Projected To Attain Around USD 45.74 Billion By 2030, Poised To Grow at a CAGR Of 18.45% During The Forecast Period 2023 To 2030. The growth is being driven by things like the increasing number of people who use the internet, smart devices, the need for surveillance and monitoring at a physical store, and government policies that encourage digitization. AI in retail is based on how businesses have been run for the past few decades. AI and analytics for big data are important parts of digital business. They can change everything, from how a business works to how a customer feels about it.\n\nRequest Sample Copy of Report \u201c Artificial Intelligence in Retail Market - Global Industry Analysis, Size, Share, Growth Opportunities, Future Trends, Covid-19 Impact, SWOT Analysis, Competition and Forecasts 2022 to 2030 \u201d, published by Contrive Datum Insights.\n\nRecent Developments:\n\nIn September 2022 , Microsoft partnered with Indian global IT company Infosys. The organizations wanted to enable companies to quickly redesign customer experiences, augment systems with cloud and data, and update processes through this alliance.\n\n, Microsoft partnered with Indian global IT company Infosys. The organizations wanted to enable companies to quickly redesign customer experiences, augment systems with cloud and data, and update processes through this alliance. In August 2022 , the company introduced a new solution for personalized e-commerce product suggestions called ViSenze's Session-Based Recommendations. With the new approach, customers would get a more personalized experience without providing any personal information.\n\n, the company introduced a new solution for personalized e-commerce product suggestions called ViSenze's Session-Based Recommendations. With the new approach, customers would get a more personalized experience without providing any personal information. In July 2022, Intel released novel reference kits. The new solution should make it easier for data scientists and engineers to understand how AI can be implemented in diverse environments, including manufacturing, retail, healthcare, and more.\n\nSegment Overview\n\nBy Offering Analysis:\n\nBased on what is being sold, artificial intelligence in the retail sector can be broken down into solutions and services. The cure is taking over the market. Smart shop, digital commerce, intelligent consumer insights, smart delivery, intelligent supply chain, and other solutions are all taken into account. Because it's getting harder to keep track of all the different retail activities, new and creative automated solutions are expected to drive the retail business. With the help of the AI-based retail solution, retailers can handle logistics, supply chain operations, warehouse management, and better customer experiences. This will likely speed up the use of AI in the retail sector.\n\nBy Function Analysis:\n\nBased on what they do, artificial intelligence in retail is divided into two groups: operations-focused and customer-facing. Focusing on operations grows a market's share of revenue. Solutions for operations are being thought about. These include Intel Movidius VPUs, Taskdesk Virtual Agents, RetailNext Store Layout, and ViSenze merchandising planning. AI is being used by retailers to improve the efficiency of their operations in areas like merchandising, logistics, supply chains, on-time delivery, and more. Effective backend management gives merchants more time to work on growing their sales and new growth projects.\n\nBy Application Analysis:\n\nSome of the applications that make up the market are predictive analytics, in-store visual monitoring and surveillance, customer relationship management (CRM), market forecasting, inventory management, and others. This market was led by predictive analytics in 2020. Some of its most important uses are labour optimization, shelf management, store operations, and dividing people into groups based on their demographics. Retailers are using AI-based predictive analysis to learn more about how customers might behave and how the market might change in the future. The AI is also used to get analysis based on many demographics, such as region, country, culture, gender, age, and others.\n\nBy Type Analysis:\n\nBased on how it works, artificial intelligence (AI) in retail is divided into two types: online and offline. The most money is made offline, and it is expected that this will stay the same during the projected time period. Customers want this technology because, among other things, it can help manage store operations, improve merchandising and assortment, and automate personalised product recommendations, all of which make shopping better for customers. Online is likely to grow quickly because more people are shopping online and virtually. Artificial intelligence technology is being used by retailers to improve their online customer service. ViSenze, for example, offers a number of smart e-commerce solutions, such as usability across devices, smart recommendations, discovery, and motivational SEO marketing.\n\nBy Technology Analysis:\n\nIn the retail sector, artificial intelligence is categorised using computing technologies like computer vision, machine learning, natural language processing, and others. During the time frame of the projection, natural language processing is expected to grow quickly. Businesses pay close attention to how clients act, how they feel, what kind of personality they have, and other factors in order to offer specialised and personalised services.\n\nRegional Outlook:\n\nWith 38.5% of the revenue in 2021, North America had the most. Given how much money is being spent on AI projects and related research and development, there is a lot of room for the industry to grow. Regional retailers are also focusing on getting as much information as they can about their customers' preferences so they can provide better customer service.\n\nGoogle Inc., Microsoft, IBM Corp., Salesforce, and Amazon Web Services are among the market leaders who use both organic and inorganic methods. For example, in January 2021, Google Cloud launched Product Discovery Solutions for retail to make online shopping more personalised.\n\nBuy this Premium Research Report@\n\nhttps://www.contrivedatuminsights.com/buy/248369/?Mode=PM\n\nScope of Report:\n\nReport Attributes Details Growth Rate CAGR of 18.45% from 2023 to 2030. Revenue Forecast by 2030 USD 45.74 Billion By Type Offline, Online, Other By Application Predictive Analytics, In-Store Visual Monitoring and Surveillance, Customer Relationship Management (CRM), Market Forecasting, Inventory Management, Others By Technology Computer Vision, Machine Learning, Natural Language Processing, Other By Companies IBM Corporation, Microsoft, SAP SE, Amazon Web Services, Oracle, Salesforce Inc., Intel, NVIDIA, Google LLC, Sentient Technology, ViSenze Regions and Countries Covered North America: (US, Canada, Mexico, Rest of North America)\n\nEurope(Germany, France, Italy, Spain, UK, Nordic Countries, Benelux Union, Rest of Europe)\n\nAsia-Pacific (Japan, China, India, Australia, South Korea, Southeast Asia, Rest of Asia-Pacific)\n\nThe Middle East & Africa(Saudi Arabia, UAE, Egypt, South Africa, Rest of the Middle East & Africa)\n\nLatin America(Brazil, Argentina, Rest of Latin America)\n\nRest Of the World Base Year 2022 Historical Year 2017 to 2022 Forecast Year 2023 to 2030\n\nLATEST TRENDS:\n\nE-commerce sites and virtual shops are being used by more and more people every day. People can search for new products in new ways today, such as by using images, videos, and their voices. Visual search makes the most of its features by processing queries and mining the metadata. This is done with the help of artificial intelligence. AI is used by the visual search engine to analyse, track, and predict growing shopping trends. This makes shopping better and keeps shoppers interested.\n\nAccording to the 2020 report from Syte Visual Conception Ltd., almost 80% of shoppers start their shopping with a visual search. So, retailers who want to improve customer service and boost sales must now have AI-based search engines. AI-powered search features also help retailers learn important things about consumer trends and make good business decisions. In the next few years, retailers can expect to get a lot of information and growth opportunities from search engines that use AI.\n\nDRIVING FACTORS:\n\nBecause they are so good at helping customers, chatbots that use artificial intelligence are becoming very popular in the retail industry. Chatbot helps customers have a better experience by giving them answers that are specific to them and fit their needs. Accenture insights found that in 2018, 91% of customers were more likely to buy from brands that gave them personalised recommendations and services.\n\nThe AI chatbots can understand what people say with the help of natural language processing (NLP) and machine learning (ML). These technologies let you know what customers want right now. It also helps the chatbot understand how customers feel and act, which helps the chatbot answer customer questions and build relationships. For example, Levi's has a platform called Levi's Virtual Stylist, which is a chatbot that makes suggestions to the customer. The bot asks users for basic information like size, fit, material, and even preferred brands so it can make quick suggestions. So, it is expected that the AI-powered chatbot will make AI more popular in the retail industry.\n\nKey Segments Covered:\n\nTop Market Players:\n\nIBM Corporation, Microsoft, SAP SE, Amazon Web Services, Oracle, Salesforce Inc., Intel, NVIDIA, Google LLC, Sentient Technology, ViSenze, and others.\n\nBy Offering\n\nSolution\n\nService\n\nBy Function\n\nOperations-Focused\n\nCustomer-Facing\n\nOther\n\nBy Type\n\nOffline\n\nOnline\n\nOther\n\nBy Application\n\nPredictive Analytics\n\nIn-Store Visual Monitoring and Surveillance\n\nCustomer Relationship Management (CRM)\n\nMarket Forecasting\n\nInventory Management\n\nOthers\n\nBy Technology\n\nComputer Vision\n\nMachine Learning\n\nNatural Language Processing\n\nOther\n\nRegions and Countries Covered\n\nNorth America: (US, Canada, Mexico, Rest of North America)\n\n(US, Canada, Mexico, Rest of North America) Europe: (Germany, France, Italy, Spain, UK, Nordic Countries, Benelux Union, Rest of Europe)\n\n(Germany, France, Italy, Spain, UK, Nordic Countries, Benelux Union, Rest of Europe) Asia-Pacific: (Japan, China, India, Australia, South Korea, Southeast Asia, Rest of Asia-Pacific)\n\n(Japan, China, India, Australia, South Korea, Southeast Asia, Rest of Asia-Pacific) The Middle East & Africa: (Saudi Arabia, UAE, Egypt, South Africa, Rest of the Middle East & Africa)\n\n(Saudi Arabia, UAE, Egypt, South Africa, Rest of the Middle East & Africa) Latin America: (Brazil, Argentina, Rest of Latin America)\n\n(Brazil, Argentina, Rest of Latin America) Rest Of the World\n\nCheck out more related studies published by Contrive Datum Insights:\n\n4D Printing Technology Market - The 4D Printing Market was valued at USD 62.02 million in 2020 and is projected to grow at a CAGR of 41.0% over the forecast period 2022 to 2030. The North American market is expected to witness strong growth during the forecast period accounting for a major share of the overall market.\n\n- The 4D Printing Market was valued at USD 62.02 million in 2020 and is projected to grow at a CAGR of 41.0% over the forecast period 2022 to 2030. The North American market is expected to witness strong growth during the forecast period accounting for a major share of the overall market. 3D & 4D Technology Market - The global 3D & 4D Technology Market size was valued at USD 254.21 Billion in 2022 and is projected to reach USD 931.17 Billion in 2030, growing at a CAGR of 20.38% from 2023 to 2030. North America and Europe have well-developed markets for 3D/4D technology. The market is expected to grow the most in the Asia-Pacific region.\n\n- The global 3D & 4D Technology Market size was valued at USD 254.21 Billion in 2022 and is projected to reach USD 931.17 Billion in 2030, growing at a CAGR of 20.38% from 2023 to 2030. North America and Europe have well-developed markets for 3D/4D technology. The market is expected to grow the most in the Asia-Pacific region. 3D Printed Jewellery Market - The Global 3D Printed Jewellery Market Was Estimated At USD 5.65 Billion In 2019 And Is Projected To Reach USD 5.65 Billion By 2026. During the time period predicted, North America will be responsible for 42% of the market's growth.\n\nCustomization of the Report: The report can be customized as per client needs or requirements.For any queries, you can contact us on anna@contrivedatuminsights.com or +1 215-297-4078. Our sales executives will be happy to understand your needs and provide you with the most suitable reports.\n\nAbout Us:\n\nContrive Datum Insights (CDI) is a global delivery partner of market intelligence and consulting services to officials at various sectors such as investment, information technology, telecommunication, consumer technology, and manufacturing markets. CDI assists investment communities, business executives, and IT professionals to undertake statistics-based accurate decisions on technology purchases and advance strong growth tactics to sustain market competitiveness. Comprising of a team size of more than 100 analysts and cumulative market experience of more than 200 years, Contrive Datum Insights guarantees the delivery of industry knowledge combined with global and country-level expertise.\n\nSocial: Facebook / LinkedIn / Twitter\n\nContact Us:\n\nAnna B. | Head Of Sales\n\nContrive Datum Insights\n\nPhone: +91 9834816757 | +1 2152974078\n\nEmail: anna@contrivedatuminsights.com",
            "title": "Artificial Intelligence in Retail Market Is Expected To Reach around USD 45.74 Billion by 2030, Grow at a CAGR Of 18.45% during Forecast Period 2023 To 2030 | Data By Contrive Datum Insights Pvt Ltd.",
            "keywords": [
                "intelligence",
                "2030",
                "market",
                "ai",
                "customer",
                "2022",
                "pvt",
                "grow",
                "technology",
                "customers",
                "retail",
                "period",
                "reach",
                "usd",
                "insights",
                "rest"
            ],
            "link": "https://www.globenewswire.com/news-release/2023/01/19/2592066/0/en/Artificial-Intelligence-in-Retail-Market-Is-Expected-To-Reach-around-USD-45-74-Billion-by-2030-Grow-at-a-CAGR-Of-18-45-during-Forecast-Period-2023-To-2030-Data-By-Contrive-Datum-In.html",
            "skip": "true",
            "gpt_score_reason": "\n\n7.5/10",
            "gpt_relevancy_score": 0
        },
        "9": {
            "text": "Therapy by chatbot? The promise and challenges in using AI for mental health\n\nEnlarge this image Ariel Davis for NPR Ariel Davis for NPR\n\nJust a year ago, Chukurah Ali had fulfilled a dream of owning her own bakery \u2014 Coco's Desserts in St. Louis, Mo. \u2014 which specialized in the sort of custom-made ornate wedding cakes often featured in baking show competitions. Ali, a single mom, supported her daughter and mother by baking recipes she learned from her beloved grandmother.\n\nBut last February, all that fell apart, after a car accident left Ali hobbled by injury, from head to knee. \"I could barely talk, I could barely move,\" she says, sobbing. \"I felt like I was worthless because I could barely provide for my family.\"\n\nAs darkness and depression engulfed Ali, help seemed out of reach; she couldn't find an available therapist, nor could she get there without a car, or pay for it. She had no health insurance, after having to shut down her bakery.\n\nSo her orthopedist suggested a mental-health app called Wysa. Its chatbot-only service is free, though it also offers teletherapy services with a human for a fee ranging from $15 to $30 a week; that fee is sometimes covered by insurance. The chatbot, which Wysa co-founder Ramakant Vempati describes as a \"friendly\" and \"empathetic\" tool, asks questions like, \"How are you feeling?\" or \"What's bothering you?\" The computer then analyzes the words and phrases in the answers to deliver supportive messages, or advice about managing chronic pain, for example, or grief \u2014 all served up from a database of responses that have been prewritten by a psychologist trained in cognitive behavioral therapy.\n\nThat is how Ali found herself on a new frontier of technology and mental health. Advances in artificial intelligence \u2014 such as Chat GPT \u2014 are increasingly being looked to as a way to help screen for, or support, people who dealing with isolation, or mild depression or anxiety. Human emotions are tracked, analyzed and responded to, using machine learning that tries to monitor a patient's mood, or mimic a human therapist's interactions with a patient. It's an area garnering lots of interest, in part because of its potential to overcome the common kinds of financial and logistical barriers to care, such as those Ali faced.\n\nPotential pitfalls and risks of chatbot therapy\n\nThere is, of course, still plenty of debate and skepticism about the capacity of machines to read or respond accurately to the whole spectrum of human emotion \u2014 and the potential pitfalls of when the approach fails. (Controversy flared up on social media recently over a canceled experiment involving chatbot-assisted therapeutic messages.)\n\nMy worry is [teens] will turn away from other mental health interventions, saying, 'Oh well, I already tried this and it didn't work.'\n\n\"The hype and promise is way ahead of the research that shows its effectiveness,\" says Serife Tekin, a philosophy professor and researcher in mental health ethics at the University of Texas San Antonio. Algorithms are still not at a point where they can mimic the complexities of human emotion, let alone emulate empathetic care, she says.\n\nTekin says there's a risk that teenagers, for example, might attempt AI-driven therapy, find it lacking, then refuse the real thing with a human being. \"My worry is they will turn away from other mental health interventions saying, 'Oh well, I already tried this and it didn't work,' \" she says.\n\nBut proponents of chatbot therapy say the approach may also be the only realistic and affordable way to address a gaping worldwide need for more mental health care, at a time when there are simply not enough professionals to help all the people who could benefit.\n\nSomeone dealing with stress in a family relationship, for example, might benefit from a reminder to meditate. Or apps that encourage forms of journaling might boost a user's confidence by pointing when out where they make progress.\n\nProponents call the chatbot a 'guided self-help ally'\n\nIt's best thought of as a \"guided self-help ally,\" says Athena Robinson, chief clinical officer for Woebot Health, an AI-driven chatbot service. \"Woebot listens to the user's inputs in the moment through text-based messaging to understand if they want to work on a particular problem,\" Robinson says, then offers a variety of tools to choose from, based on methods scientifically proven to be effective.\n\nMany people will not embrace opening up to a robot.\n\nChukurah Ali says it felt silly to her too, initially. \"I'm like, 'OK, I'm talking to a bot, it's not gonna do nothing; I want to talk to a therapist,\" Ali says, then adds, as if she still cannot believe it herself: \"But that bot helped!\"\n\nAt a practical level, she says, the chatbot was extremely easy and accessible. Confined to her bed, she could text it at 3 a.m.\n\n\"How are you feeling today?\" the chatbot would ask.\n\n\"I'm not feeling it,\" Ali says she sometimes would respond.\n\nThe chatbot would then suggest things that might soothe her, or take her mind off the pain \u2014 like deep breathing, listening to calming music, or trying a simple exercise she could do in bed. Ali says things the chatbot said reminded her of the in-person therapy she did years earlier. \"It's not a person, but, it makes you feel like it's a person,\" she says, \"because it's asking you all the right questions.\"\n\nTechnology has gotten good at identifying and labeling emotions fairly accurately, based on motion and facial expressions, a person's online activity, phrasing and vocal tone, says Rosalind Picard, director of MIT's Affective Computing Research Group. \"We know we can elicit the feeling that the AI cares for you,\" she says. But, because all AI systems actually do is respond based on a series of inputs, people interacting with the systems often find that longer conversations ultimately feel empty, sterile and superficial.\n\nWhile AI may not fully simulate one-on-one individual counseling, its proponents say there are plenty of other existing and future uses where it could be used to support or improve human counseling.\n\nAI might improve mental health services in other ways\n\n\"What I'm talking about in terms of the future of AI is not just helping doctors and [health] systems to get better, but helping to do more prevention on the front end,\" Picard says, by reading early signals of stress, for example, then offering suggestions to bolster a person's resilience. Picard, for example, is looking at various ways technology might flag a patient's worsening mood \u2014 using data collected from motion sensors on the body, activity on apps, or posts on social media.\n\nTechnology might also help improve the efficacy of treatment by notifying therapists when patients skip medications, or by keeping detailed notes about a patient's tone or behavior during sessions.\n\nMaybe the most controversial applications of AI in the therapy realm are the chatbots that interact directly with patients like Chukurah Ali.\n\nWhat's the risk?\n\nChatbots may not appeal to everyone, or could be misused or mistaken. Skeptics point to instances where computers misunderstood users, and generated potentially damaging messages.\n\nBut research also shows some people interacting with these chatbots actually prefer the machines; they feel less stigma in asking for help, knowing there's no human at the other end.\n\nAli says that as odd as it might sound to some people, after nearly a year, she still relies on her chatbot.\n\n\"I think the most I talked to that bot was like 7 times a day,\" she says, laughing. She says that rather than replacing her human health care providers, the chatbot has helped lift her spirits enough so she keeps those appointments. Because of the steady coaching by her chatbot, she says, she's more likely to get up and go to a physical therapy appointment, instead of canceling it because she feels blue.\n\nThat's precisely why Ali's doctor, Washington University orthopedist Abby Cheng, suggested she use the app. Cheng treats physical ailments, but says almost always the mental health challenges that accompany those problems hold people back in recovery. Addressing the mental-health challenge, in turn, is complicated because patients often run into a lack of therapists, transportation, insurance, time or money, says Cheng, who is conducting her own studies based on patients' use of the Wysa app.\n\n\"In order to address this huge mental health crisis we have in our nation \u2014 and even globally \u2014 I think digital treatments and AI can play a role in that, and at least fill some of that gap in the shortage of providers and resources that people have,\" Cheng says.\n\nNot meant for crisis intervention\n\nBut getting to such a future will require navigating thorny issues like the need for regulation, protecting patient privacy and issues of legal liability. Who bears responsibility if the technology goes wrong?\n\nMany similar apps on the market, including those from Woebot or Pyx Health, repeatedly warn users that they are not designed to intervene in acute crisis situations. And even AI's proponents argue computers aren't ready, and may never be ready, to replace human therapists \u2014 especially for handling people in crisis.\n\n\"We have not reached a point where, in an affordable, scalable way, AI can understand every sort of response that a human might give, particularly those in crisis,\" says Cindy Jordan, CEO of Pyx Health, which has an app designed to communicate with people who feel chronically lonely.\n\nThe hype and promise is way ahead of the research that shows its effectiveness.\n\nJordan says Pyx's goal is to broaden access to care \u2014 the service is now offered in 62 U.S. markets and is paid for by Medicaid and Medicare. But she also balances that against worries that the chatbot might respond to a suicidal person, \" 'Oh, I'm sorry to hear that.' Or worse, 'I don't understand you.' \" That makes her nervous, she says, so as a backup, Pyx staffs a call center with people who call users when the system flags them as potentially in crisis.\n\nWoebot, a text-based mental health service, warns users up front about the limitations of its service, and warnings that it should not be used for crisis intervention or management. If a user's text indicates a severe problem, the service will refer patients to other therapeutic or emergency resources.\n\nCross-cultural research on effectiveness of chatbot therapy is still sparse\n\nAthena Robinson, chief clinical officer for Woebot, says such disclosures are critical. Also, she says, \"it is imperative that what's available to the public is clinically and rigorously tested,\" she says. Data using Woebot, she says, has been published in peer-reviewed scientific journals. And some of its applications, including for post-partum depression and substance use disorder, are part of ongoing clinical research studies. The company continues to test its products' effectiveness in addressing mental health conditions for things like post-partum depression, or substance use disorder.\n\nBut in the U.S. and elsewhere, there is no clear regulatory approval process for such services before they go to market. (Last year Wysa did receive a designation that allows it to work with Food and Drug Administration on the further development of its product.)\n\nIt's important that clinical studies \u2014 especially those that cut across different countries and ethnicities \u2014 continue to be done to hone the technology's intelligence and its ability to read different cultures and personalities, says Aniket Bera, an associate professor of computer science at Purdue.\n\n\"Mental-health related problems are heavily individualized problems,\" Bera says, yet the available data on chatbot therapy is heavily weighted toward white males. That bias, he says, makes the technology more likely to misunderstand cultural cues from people like him, who grew up in India, for example.\n\n\"I don't know if it will ever be equal to an empathetic human,\" Bera says, but \"I guess that part of my life's journey is to come close.\"\n\nAnd, in the meantime, for people like Chukurah Ali, the technology is already a welcome stand-in. She says she has recommended the Wysa app to many of her friends. She says she also finds herself passing along advice she's picked up from the app, asking friends, \"Oh, what you gonna do today to make you feel better? How about you try this today?\"\n\nIt isn't just the technology that is trying to act human, she says, and laughs. She's now begun mimicking the technology.",
            "title": "Therapy by chatbot? The promise and challenges in using AI for mental health",
            "keywords": [
                "chatbot",
                "challenges",
                "human",
                "therapy",
                "users",
                "ali",
                "patients",
                "health",
                "using",
                "technology",
                "ai",
                "mental",
                "promise"
            ],
            "link": "https://www.npr.org/sections/health-shots/2023/01/19/1147081115/therapy-by-chatbot-the-promise-and-challenges-in-using-ai-for-mental-health",
            "skip": "false",
            "gpt_score_reason": "\n\n7/10",
            "gpt_relevancy_score": 7,
            "summary": "\n\nSummary: Advances in Artificial Intelligence, such as Chat GPT are increasingly being looked at as a way to help support people dealing with mental health issues. AI has become good at identifying and labeling emotions, based on expressions and online activity, but still cannot fully simulate one-on-one individual counseling. Despite controversy over using chatbot therapy for mental health care, proponents say it is the only realistic and affordable way to address the worldwide gap in access to care. Studies have been done to hone its ability to read different cultures and personalities; however cross-cultural research is still sparse."
        },
        "10": {
            "text": "Prostock-Studio / iStock.com\n\nThe banks of 2028 won\u2019t be unrecognizable compared to those you know today, but they will provide a smoother, simpler and faster customer experience \u2014 and they\u2019ll do it with fewer humans.\n\nSee the List: GOBankingRates\u2019 Best Banks of 2023\n\nCheck Out: Here\u2019s How Much Americans Have in Their Savings Accounts in 2023\n\nRead: GOBankingRates\u2019 Best Checking Accounts 2023\n\n\u201cOver the next five years, the banking industry is set to experience a wide array of changes that will revolutionize and influence how customers interact with their banks,\u201d said small-business and investor consultant Lucas \u201cLuke\u201d Solomon, founder of the smart banking site FX4Biz. \u201cMany of these changes can be attributed to the massive influx of emerging technologies such as big data analytics, artificial intelligence (AI), machine learning (ML), blockchain, cloud computing, and robotic process automation (RPA).\u201d\n\nBut the industry is also primed for changes beyond just the exciting sci-fi stuff. Banks are already reconsidering how they fit into their customers\u2019 lives in order to succeed in a rapidly changing landscape.\n\nNew Technology Will Lend a Human Touch to Automated Banking\n\nThe rise of \u201cpress one for this, press two for that\u201d automated banking saved the industry a bundle of money, but it has been largely underwhelming in terms of customer experience. But a new generation of technology is bringing the human element back to automated banking without the need for actual humans \u2014 at least not many of them.\n\n\u201cOne trend that we are already starting to see is the increased use of artificial intelligence and machine learning in banking,\u201d said Fluent in Finance founder Andrew Lokenauth, an investing and banking professional. \u201cThese technologies are being used to enhance customer service, improve risk management and streamline internal operations.\u201d\n\nSounds promising \u2014 but what does that mean exactly?\n\nA small region credit union in the Midwest offers a glimpse into the future. Take a look.\n\nStory continues\n\nThe MLCU Model: Using Tech To Conserve Resources and Reimagine the Branch Experience\n\nThe biggest banks have the resources to develop robust digital infrastructure while also maintaining a full-time brick-and-mortar presence \u2014 but many smaller institutions do not. They\u2019ll survive the next five years only if they use emerging technologies like AI and ML to supplement their limited human resources and create a hybrid branch model.\n\nFor example, instead of migrating to a fully online platform, Michigan Legacy Credit Union (MLCU) is keeping its physical locations but transforming them into teller-less branches where smart ATMs do the heavy lifting. These next-level ATMs can facilitate standard deposits and withdrawals, but they\u2019re also fitted with a video teller platform that lets members open accounts, process fraud disputes, perform account maintenance like address changes and apply for loans or even close them out.\n\n\u201cWith the rise of neobanks, combined with an increasing number of customers willing to handle their personal finances remotely as a result of the pandemic, brick-and-mortar banking is experiencing disruption, and the traditional bank teller and ATM experience is no longer enough,\u201d said MLCU president Carma Peters.\n\nMLCU\u2019s website now lists the five original locations and a sixth \u201cvirtual branch.\u201d\n\nTake Our Poll: How Big of a Sign-Up Bonus Would It Take for You To Change Banks?\n\nBehind-the-Scenes Grunt Work? There\u2019s a Machine for That\n\nCustomers interfacing with digital tellers to submit loan applications through smart ATMs in branches with no human employees is just the most visible application for emerging technologies like AI and ML. But most of the work that banks will farm out to robots in the coming years won\u2019t be nearly as dramatic or apparent to customers.\n\n\u201cAI- and ML-based tools can automate mundane tasks within banks, such as identifying account fraud and credit-risk analysis,\u201d Solomon said. \u201cRPA will allow financial institutions to transfer data faster than ever before while reducing costs associated with manual labor.\u201d\n\nIt\u2019s not that AI will replace humans in the banking industry. Instead, it will free them up to focus on their human customers.\n\n\u201cThe future of banking will take shape by implementing a machine-learning platform that accelerates funding times, improves loan margins and reduces regulatory risk for banks, credit unions and fintech lenders,\u201d said Will Robinson, CEO of Encapture, which provides machine-learning services for banks and lenders. \u201cThis empowers banks to add in new forms of automation technology and allows their staff to focus more on customer experience and spend less time on manual tasks.\u201d\n\nRobots as Masters of Regulatory Compliance\n\nAs an example, Robinson cited regulations like Section 1071 of the Dodd-Frank Act and the expansion of the Community Reinvestment Act.\n\nCurrently, most banks and lenders have to hire extra compliance staff to keep up with the strict reporting standards that those regulations require \u2014 but in the next five years, more banks will be able to leave it to the machines.\n\n\u201cA document-focused machine-learning platform can read and extract data from any document, eliminating manual data scrubbing for banks,\u201d Robinson said. \u201cWith this automation in place, banks are able to focus on customer experience and data integrity, rather than the quarterly compliance and regulatory audits.\u201d\n\nBanks Will Unbundle Their Services \u2014 Or Lose To Fintech Specialists\n\nGoran Luledzija, CEO of fintech SaaS provider Localizely, believes that banks will forfeit the future only if the industry continues its commitment to a one-size-fits-all philosophy. To Luledzija, the local bank as the place where you keep your savings, handle your checking, apply for a mortgage and take out an auto loan all in one place is a 20th-century model that is already fading into obscurity.\n\n\u201cIt\u2019s precisely this lack of granularity that\u2019s costing the traditional banking model the most,\u201d Luledzija said. \u201cCurrently, banks are holding onto the outdated idea that they need to be a one-stop shop for all things finance. In the meantime, different fintechs offering highly granular services are snatching customers away from banks and providing them with the exact same services, only faster and without strings attached.\u201d\n\nLike the traditional department store, the traditional bank is by no means dead \u2014 but it will die if it doesn\u2019t restructure those traditions for the modern era.\n\n\u201cIn my opinion, banks can still make it,\u201d Luledzija said. \u201cThey have immeasurable wealth in assets that they can leverage to get ahead of the competition and step away from last-century business models. It\u2019s only a matter of realizing they have to before it\u2019s too late.\u201d\n\nMore From GOBankingRates\n\nThis article originally appeared on GOBankingRates.com: Experts Predict: 3 Major Changes That Will Happen to Banking in the Next Five Years",
            "title": "Experts Predict: 3 Major Changes That Will Happen to Banking in the Next Five Years",
            "keywords": [
                "traditional",
                "human",
                "changes",
                "predict",
                "customer",
                "experience",
                "happen",
                "data",
                "banks",
                "technologies",
                "banking",
                "experts",
                "major",
                "industry"
            ],
            "link": "https://news.yahoo.com/experts-predict-3-major-changes-120023160.html",
            "skip": "true",
            "gpt_score_reason": "\n\n4/10",
            "gpt_relevancy_score": 4
        },
        "11": {
            "text": "We have a long way to go before women share equally in the creation of the technologies that will change the future.\n\nChat GPT is a type of artificial intelligence that uses natural language processing and machine learning to generate human-like conversations. It is used in a variety of applications, such as customer service chatbots, virtual assistants and automated customer support systems. Women are under-represented by a significant margin in AI and data science. (Donato Fasano / Getty Images)\n\nNew advancements in data science often spark dire predictions about how powerful new technologies will transform the world. Yet, as writer Stephen Shankland reminds us, technologies like Open AI\u2019s new Chat GPT (short for chat-based Generative Pretrained Transformer) are created by humans.\n\nChat GPT is a chatbot that is \u201ctrained with human assistance to deliver more useful, better dialog.\u201d The people assisting that training\u2014those who create the models and assemble the data used to train chatbots\u2014make a difference in the technologies that will go on to shape our lives.\n\nComputer scientist Joy Buolamwini, an early critic of racial bias in facial recognition software, said technology should \u201cbe more attuned to the people who use it and the people it\u2019s used on.\u201d But as long as the field of data science remains predominantly male and white, it will be difficult to have inclusive advancements in artificial intelligence. A new white paper by Women in Data Science (WiDS) at Stanford University, \u201cIdentifying and Removing Barriers for Women to Pursue Graduate Degrees in Data Science and AI,\u201d shows that we have a long way to go before women share equally in the creation of the technologies that will change the future.\n\nWhy, despite decades of pipeline programs created to address the lack of gender diversity in computer and information sciences, are only 17 percent of those enrolled in Ph.D. programs women? The white paper contends that the proverbial pipeline is not so much leaky as it is blocked by barriers that prevent women from pursuing graduate studies in data science and AI (the authors focus on graduate programs since most successful data scientists today have advanced degrees).\n\nSome of these barriers are frustratingly familiar. Chief among these is a lack of awareness among undergraduates about academic programs that can lead to careers in data science, a related lack of awareness of the value of a graduate degree in data science (the median entry-level annual salary is $95,000), and the lack of awareness of the impact women can have on society as data scientists.\n\nBecause there are so few women in the field, undergraduate women rarely see role models who can help them imagine careers in data science and graduate students cannot work with women mentors who can support them as they complete their degrees.\n\nA lack of family, peer and community support also presents barriers for women. Women who are expected to provide financial support to their families, are low-income, or are first-generation students may not receive support from communities that are also unlikely to be aware of the value of a graduate degree. First-generation students in particular do not have access to the tools they will need to navigate academic programs in which they are a minority.\n\nA male-dominated data science culture paired with a lack of female role models can also create an environment conducive to self-doubt and impostor syndrome for young, female data scientists. Self-efficacy is the belief that you can accomplish desired goals and objectives, even when presented with adversity. As the WiDS white paper shows, high-achieving female students report lower self-efficacy in male-dominated STEM fields relative to both their male peers and average-performing male students.\n\nBelieving that they are unfit and unwelcome in an academic program creates feelings of inadequacy among female data scientists, with negative impacts on their confidence and self-esteem. Capable and talented women dealing with low self-efficacy and impostor syndrome often leave data science and related fields at the undergraduate level, resulting in under-enrollment in graduate programs. They also leave programs at higher rates before completing a degree. Internal barriers can be just as harmful as external barriers, but they are perhaps easier to overcome with support and inclusion from mentors and peers.\n\nWhen data scientists do not bring diverse perspectives to their work, the science and technology they produce suffers.\n\nSome solutions have been proposed and implemented at the undergraduate level to bridge the gender gap in advanced degree enrollment in data science. The white paper suggests that a critical threshold of at least 30 percent of those working in computer and information science should identify as women to realistically enforce equity and inclusion initiatives within the field.\n\nIn order to reach this threshold within the next decade, WiDS has proposed the creation of the Women in Data Science Academy, a cross-university program that intends to both increase awareness of the avenues available to pursue advanced degrees in data science, as well as encourage women to forge a direct path from their undergraduate to graduate studies.\n\nThe WiDS Academy plans to pair technical-skills-based initiatives to strengthen coding literacy with career-oriented programs like graduate school recruitment initiatives and career talks with women working and conducting research in the data science field. These initiatives are designed to increase awareness and self-efficacy and try to decrease feelings of impostor syndrome among undergraduate students. Although this academy alone will not be able to address the barriers that women face in this field, by presenting a path for talented and qualified women to pursue advanced data science degrees and a model for other institutions to adopt, it might yet be possible to reach the critical threshold of 30 percent within the decade.\n\nAs the white paper shows, the representation of women in data science worldwide is \u201cdismal.\u201d When data scientists do not bring diverse perspectives to their work, the science and technology they produce suffers. Not only are women being prevented from entering a dynamic and high-paying field, but the lack of diversity also results in a loss of creativity and innovation in data science. Lack of inclusion hurts women and the subsequent lack of diversity hurts science. The white paper offers us solutions.\n\nUp next:\n\nU.S. democracy is at a dangerous inflection point\u2014from the demise of abortion rights, to a lack of pay equity and parental leave, to skyrocketing maternal mortality, and attacks on trans health. Left unchecked, these crises will lead to wider gaps in political participation and representation. For 50 years, Ms. has been forging feminist journalism\u2014reporting, rebelling and truth-telling from the front-lines, championing the Equal Rights Amendment, and centering the stories of those most impacted. With all that\u2019s at stake for equality, we are redoubling our commitment for the next 50 years. In turn, we need your help, Support Ms. today with a donation\u2014any amount that is meaningful to you. For as little as $5 each month, you\u2019ll receive the print magazine along with our e-newsletters, action alerts, and invitations to Ms. Studios events and podcasts. We are grateful for your loyalty and ferocity.",
            "title": "The Lack of Women Data Scientists Hurts Artificial Intelligence",
            "keywords": [
                "intelligence",
                "scientists",
                "graduate",
                "barriers",
                "women",
                "science",
                "programs",
                "artificial",
                "support",
                "paper",
                "data",
                "lack",
                "hurts",
                "white"
            ],
            "link": "https://msmagazine.com/2023/01/19/women-data-science-technology-artificial-intelligence-chatbot/",
            "skip": "false",
            "gpt_score_reason": " \n\n7/10",
            "gpt_relevancy_score": 7,
            "summary": "\n\nSummary: As OpenAI's new Chat GPT technology is created by humans, the white paper from Women in Data Science (WiDS) at Stanford University highlights that women are significantly under-represented in AI and data science. Through a lack of family, peer and community support, a lack of role models within the field and male-dominated culture, self-doubt and impostor syndrome become barriers for young female data scientists. WiDS has proposed solutions such as the Women in Data Science Academy which presents an opportunity to bridge gender gaps within advanced degree enrollment of data science. Increasing diversity not only gives more opportunities to women but also drives innovation in data science."
        },
        "12": {
            "text": "Last week, Reed Wilen, an elite gamer who uses the handle \u201cChicago\u201d in Rocket League, a popular vehicular-soccer game, encountered a strange and troubling new opponent. The player seemed like a novice at first, moving their rocket-powered vehicle in a hesitant and awkward way. Then they caught and balanced the ball perfectly on the hood of their car, and dribbled it with superhuman skill towards the goal at high speed.\n\nNot only was the other driver clearly a bot\u2014it was also ridiculously good. \u201cIt is very confusing to play against,\u201d Wilen says. \u201cIts perfect dribbling would cause havoc on almost every player.\u201d\n\nWilen is one of a number of elite Rocket League players to have recently encountered the bot in competitive play. It is not yet good enough to beat all comers, but it can play to a high level, allowing less skilled players to cheat their way to a higher ranking.\n\nRocket League is frenetic and extremely tricky to play. Each player controls a car capable of impossible acrobatics inside an arena where gravity and physics are apparently set to ludicrous mode. The objective is to use your vehicle to maneuver a giant ball past your opponent and into their goal, a task that requires considerable skill and patience. Sometimes two players work together as a team, making huge leaps, desperate parries, and accidentally colliding, all while trying to anticipate and counter their opponents' own antics.\n\nTop Rocket League players will often launch their cars through the air to move the ball toward the goal, but Wilen says the bot he faced appears to have been trained specifically to carry it on the ground. \u201cThe bot doesn't really flip around too often and doesn't jump in the air,\u201d he says, apparently because it hasn\u2019t been programmed to, or learned how to do so. \u201cInstead, it waits for the ball to come down, where it catches it on top of the car and performs a perfect dribble towards the opposing team\u2019s net,\u201d Wilen says.\n\nThe bot that Wilen and others have come up against is called Nexto. It picked up the ability to dribble and score using an artificial intelligence approach known as reinforcement learning, which has underpinned research breakthroughs that let computers master other difficult games such as Go and Starcraft. The technique has also been applied to more practical areas, including chip design and data center cooling in recent years. Reinforcement learning entails creating a program that can perform a task at a basic level and improve by responding to feedback as it practices.\n\nThe company behind Rocket League, Psyonix, part of Epic Games, allows players to deploy bots to practice against. In 2020 it made an application programming interface (API) available to help developers build bots more easily. Last April, a group of Rocket League enthusiasts with coding skills announced RLGym, an open source library for building reinforcement-learning bots for Rocket League. Later in the year, the group released several open source AI bots\u2014including an especially skilled dribbler called Nexto.",
            "title": "Cheaters Hacked an AI Bot\u2014and Beat the Rocket League Elite",
            "keywords": [
                "cheaters",
                "bot",
                "wilen",
                "hacked",
                "elite",
                "botand",
                "play",
                "players",
                "league",
                "goal",
                "ball",
                "beat",
                "rocket",
                "ai",
                "car",
                "bots"
            ],
            "link": "https://www.wired.com/story/cheaters-hacked-an-ai-bot-and-beat-the-rocket-league-elite/",
            "skip": "false",
            "gpt_score_reason": "\n\n6/10",
            "gpt_relevancy_score": 6,
            "summary": "\n\nSummary: Elite gamers playing popular game Rocket League have recently been coming up against a sophisticated bot called Nexto, which uses reinforcement learning to improve its skills. The bot is extremely skilled at dribbling the ball on the ground and scoring goals in competitive play, making it a powerful cheating tool for less skilled players. Companies like Psyonix are allowing people to use bots as practice tools but now developers with coding skills have access to an open source library called RLGym that helps create even more advanced bots."
        },
        "13": {
            "text": "SYLVIE DOUGLIS, BYLINE: NPR.\n\n(SOUNDBITE OF DROP ELECTRIC SONG, \"WAKING UP TO THE FIRE\")\n\nGREG ROSALSKY, HOST:\n\nWhat if in high school, instead of spending hours working on an essay about the American Revolution or - I don't know - like, \"Catcher In The Rye,\" you could just have a machine do it?\n\nEMMA PEASLEE, HOST:\n\nIt's basically every kid's dream. And now there's a new artificial intelligence chatbot that's making that dream a reality.\n\nROSALSKY: You've probably heard about this technology. It's called ChatGPT, and it could do more than just do your homework. It could write code for a website, formulate a movie pitch, help you with your mental health, even haggle with Comcast to lower your bill.\n\nPEASLEE: All you have to do is give it a prompt like write me a college essay about the Civil War. And within seconds, boom, you have a somewhat decent essay about the Civil War.\n\nROSALSKY: And while kids around the world may be celebrating, like, the end of homework, this technology raises some serious questions for our education system. Like, what is the point of learning to write essays at school when AI can now do that for us? I'm Greg Rosalsky.\n\nPEASLEE: And I'm Emma Peaslee. And we're taking over the show to talk about a new technology that not too long ago might have seemed like science fiction.\n\nROSALSKY: Today on THE INDICATOR - how the AI revolution could reshape everything from education to how we communicate with each other and the story of one college kid who has developed a tool that maybe makes our AI future just a little less bleak.\n\n(SOUNDBITE OF CHRIS JONES SONG, \"NO ESCAPE\")\n\nROSALSKY: While many college kids were having fun on winter break, 22-year-old Edward Tian was feverishly working on a new app, an app to combat misuse of a powerful new artificial intelligence tool called ChatGPT.\n\nEDWARD TIAN: I think we're absolutely at, like, an inflection point. This technology is incredible, and I do believe it is, like, the future. But at the same time, it's like we're opening a Pandora's box. And we needed safeguards to basically adopt it responsibly.\n\nPEASLEE: Most of us have probably heard of this by now. ChatGPT is an interactive chatbot that was released in late November by a company called OpenAI. The chatbot is powered by machine learning. And I have to say, when I first heard of it, I was impressed, but also kind of freaked out.\n\nROSALSKY: Edward had a similar reaction. And he's sort of the perfect person to take on this issue. He studies computer science and journalism at Princeton, and he's been researching how to identify text written by AI systems. But even when he first used ChatGPT, he was like, whoa, hello.\n\nTIAN: Me and a lot of my college friends were just asking it to write poems about each other, and we were like - going like, write a rap about someone and write a poem about someone. And it was like, wow, the results are pretty good.\n\nPEASLEE: Because ChatGPT is more user-friendly than past AI systems and it's free, it's been able to break through in a way that other AI systems haven't been able to.\n\nTIAN: Every teacher, like, every student - everybody was talking about ChatGPT on campus. So there was a sense of, like, oh, wow, this is everywhere.\n\nPEASLEE: And students kind of immediately realized, whoa, this thing can do a lot of homework for us. In early December, The Atlantic magazine even declared that the college essay is dead.\n\nROSALSKY: Pour one out for the college essay. So, yeah, the fall semester ends, and Edward travels home to Toronto for the holidays as the buzz around ChatGPT is really exploding. And at first, he does the typical winter break stuff. He hung out with his family. He watched \"Glass Onion\" on Netflix. But he couldn't stop thinking about this crazy new technology.\n\nPEASLEE: A crazy new technology that can make it hard to figure out whether something has been written by a human or not. And Edward thinks this is a problem not just for, like, kids copying and pasting their homework, but also things like propaganda generated by nefarious actors.\n\nTIAN: Humans deserve to know when something is written by a human or written by a machine.\n\nPEASLEE: So on holiday break, Edward decides to create an app to try and address that.\n\nTIAN: I just had so much free time over winter break, and I was getting really bored. So I was like, wow, why don't I just code this out so the world can actually use it?\n\nROSALSKY: I mean, we've all been there. Why not just create an app over winter break?\n\nPEASLEE: Totally. So on January 2, Edward released his app. He named it GPTZero. The app basically uses ChatGPT against itself. And he was able to do this because he could tap into the data of an earlier open source version. So when GPTZero analyzes a text, it can scan to see if it recognizes any of the same patterns that ChatGPT would generate.\n\nTIAN: So we're basically taking one of these, like, text generation models and asking it, hey, is this new piece of text - does it seem, like, pretty familiar to you? Like, would you probably generate it yourself?\n\nPEASLEE: When Edward went to bed that night, he didn't expect much from the app. But the next morning, his phone had blown up. He had so many texts and DMs from journalists, principals, teachers, you name it, from places as far away as France and Switzerland. It became so popular that the app crashed.\n\nROSALSKY: Before all this, Edward's biggest plans were graduating from college and getting his wisdom teeth pulled. Now he's fielding calls from venture capital firms, education leaders and global media outlets.\n\nTIAN: My own high school English teacher reached out to me. My, like, principal reached out. They're telling me, like, they're in, like, a big group chat of, like, high school principals and teachers that are all trying to talk through how to handle ChatGPT.\n\nPEASLEE: But not everyone thinks ChatGPT is a problem. Some prominent techies are even celebrating it as the end of homework. And a lot of this talk is probably hype. But it really does feel like we've entered a new world where we're being forced to reevaluate our education system and even the value of teaching kids how to write.\n\nROSALSKY: But, you know, like, some of you out there may be too young to remember a time before everyone had cellphones. But I remember those times. And because of that, I remember having to memorize phone numbers. Now I don't have to do that, and I don't do that.\n\nPEASLEE: And a techno-optimist might be like, yeah, that's for the best. It frees our minds to concentrate on other matters, but not that helpful if you, say, lose your phone and need to call someone.\n\nROSALSKY: Now with ChatGPT, it's possible to imagine, like, this dystopian future where all written communication is written by a machine, a world where everything we write to each other is like a Hallmark card written without our, you know, personality or ideas or emotions.\n\nPEASLEE: But at least when you give people Hallmark cards, they know it's from Hallmark. If you use ChatGPT to write your friend a congratulations or an apology, they might not even know it was written by a machine.\n\nROSALSKY: Which brings us to the other purpose that Edward envisions for his app - to identify and incentivize originality in human writing.\n\nTIAN: There is, like, a celebration of individuality in writing, and we're losing that individuality if we stop teaching writing at schools. There are aspects and beauty in human writing that computers never and should never co-opt. And it feels like that might be at risk if everybody is using ChatGPT to write.\n\nPEASLEE: But Edward is no Luddite. He's not trying to, like, stop AI in its tracks. He actually opposes blanket bans of the technology like the recent bans by New York City and Seattle Public Schools.\n\nTIAN: Banning it is not going to stop the wave of AI that's inevitably going to come. Like, even if you ban the ChatGPT usage on school Wi-Fis, kids might use it at home. It's like you can't stop something that's going to be inevitable. So we can't enter this future blindly.\n\nROSALSKY: As for his plans after college, Edward says the excitement and the clear demand for his new app have convinced him that he should concentrate on making it better and more accurate. Like, he's thinking about pursuing this full time.\n\nTIAN: Top priority is building this out into something that teachers can use, like, day to day in their workflow. So if you're a teacher or an educator, I'd love to talk to you. Our team, which is, right now, just me and my best friend from college, who just joined yesterday, would love to talk to you.\n\n(SOUNDBITE OF MUSIC)\n\nPEASLEE: So if I ever get a thoughtful card from you, Greg, I'm probably going to run it through ChatGPTZero (ph).\n\nROSALSKY: (Imitating robot) You are my favorite colleague, Emma. This is not written by a machine.\n\n(SOUNDBITE OF CHRIS JONES SONG, \"NO ESCAPE\")\n\nPEASLEE: This episode was produced by Brittany Cronin and engineered by Debbie Daughtry. Sierra Juarez checks the facts. Viet Le is our senior producer. Kate Concannon edits the show. And THE INDICATOR is a production of NPR.\n\n(SOUNDBITE OF CHRIS JONES SONG, \"NO ESCAPE\")\n\nCopyright \u00a9 2023 NPR. All rights reserved. Visit our website terms of use and permissions pages at www.npr.org for further information.\n\nNPR transcripts are created on a rush deadline by an NPR contractor. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR\u2019s programming is the audio record.",
            "title": "Was this text written by ChatGPT artificial intelligence? : The Indicator from Planet Money : NPR",
            "keywords": [
                "intelligence",
                "written",
                "chatgpt",
                "edward",
                "kids",
                "planet",
                "indicator",
                "artificial",
                "know",
                "write",
                "money",
                "college",
                "technology",
                "app",
                "ai",
                "npr",
                "text"
            ],
            "link": "https://www.npr.org/transcripts/1149834091",
            "skip": "false",
            "gpt_score_reason": "\n\n7/10",
            "gpt_relevancy_score": 7,
            "summary": "\n\nSummary: Edward Tian is a 22-year-old college student who has developed an app called GPTZero to combat misuse of the artificial intelligence tool ChatGPT. The technology could be used for various purposes, from writing essays and code to helping with mental health. However, it raises questions about the value of learning writing skills in school and blurring the line between machine-generated texts and those written by people. Despite some seeing this as a negative development, Edward views it optimistically as a way to incentivize originality in human writing."
        },
        "14": {
            "text": "Check out all the on-demand sessions from the Intelligent Security Summit here.\n\nThe potential for artificial intelligence (AI) and machine learning (ML) to improve human health cannot be understated, but it does face challenges.\n\nAmong the big challenges is dealing with siloed data sources, so researchers are not able to easily analyze data from multiple locations and initiatives, while still preserving privacy. It\u2019s a challenge that can potentially be solved with an approach known as federated learning.\n\nToday in a research report first published in Nature Medicine, AI biotech vendor Owkin has revealed just how powerful the federated model can be for healthcare. Owkin working alongside researchers at four hospitals in France was able to build a model with its open source technology that it claims will have a significant impact on the ability to help effectively treat breast cancer. The Owkin AI models were able to identify accurately novel biomarkers that could lead to improved personalized medical care.\n\n\u201cOwkin is an AI biotech company and we really have this ambitious goal, which is to cure cancer,\u201d Jean du Terrail, senior machine learning scientist at Owkin, told VentureBeat. \u201cWe are trying to leverage the power of AI and machine learning, in addition to our network of partners, to move towards this goal.\u201d\n\nEvent Intelligent Security Summit On-Demand Learn the critical role of AI & ML in cybersecurity and industry specific case studies. Watch on-demand sessions today. Watch Here\n\nOwkin is one of the hottest biotech startups in the market today. The company raised $80 million in funding back in June 2022, from pharmaceutical giant Bristol Myers Squibb, bringing total funding to the unicorn startup, over $300 million since the company was founded in 2016.\n\nWhy federated learning is critical for the advancement of AI healthcare\n\nIn healthcare and clinical studies, there is often a significant amount of personally identifiable information that needs to be protected and kept private. Researchers as well as hospitals will also often be required to keep some data within their own organizations, which can lead to information silos and collaboration friction.\n\nTerrail explained that federated learning provides an approach by which ML training can occur across the different information silos on patient data located in hospitals and research centers. He emphasized that the approach that Owkin has developed does not require that data ever actually leaves the source facility and patient privacy is protected.\n\nThe federated learning approach is an alternative to using synthetic data, which is also commonly used in healthcare to help protect privacy. Terrail explained that federated learning enables researchers to access real world data that is secured behind firewalls and is often difficult to access. In contrast, synthetic data is simulated data that potentially may not be entirely representative of what can be found in the real world. The risk with synthetic data in Terrail\u2019s view is that AI algorithms built with it could potentially not be accurate.\n\nTo protect patient privacy, the Owkin approach involves having data going through a process known as pseudonymization. Terrail explained that the pseudonymization process basically removes any personally identifiable information.\n\nThe open source software that enables federated learning\n\nOwkin developed a technology stack for federated learning called Substra, that is now open source. The Substra project is currently hosted by the Linux Foundation\u2019s AI and Data Initiative.\n\nTerrail said that the Substra platform enables data engineers in hospitals to connect sources remotely for the ML training. He referred to Substra as a \u2018PyTorch on steroids\u2019 application that enables researchers to add capabilities on top of existing machine learning frameworks, such as PyTorch. The additional capabilities enable the federated learning model approach, where data is located securely and privately in disparate locations.\n\nThe Substra technology also makes use of the open source Hyperledger immutable ledger blockchain technology. The Hyperledger technology enables Substra and Owkin to be able to accurately track all the data that is used. Terrail said that Hyperledger is what enables traceability into every operation that is done with Substra, which is critical to ensuring the success of clinical efforts. With traceability, researchers can verify all the steps and data that was used. Additionally it helps with enabling interpretable AI as the data doesn\u2019t all just reside in a black box that no one can audit.\n\nImproving breast cancer treatment with federated learning\n\nThe Owkin teams worked with researchers across four hospitals, and were able to train the federated learning model on clinical information and pathology data from 650 patients.\n\n\u201cWe trained the model to predict the response of the patient to neoadjuvant chemotherapy, which is the gold standard,\u201d Terrail said. \u201cIt\u2019s basically what you give to triple negative breast cancer patients that are in the early stage, but you don\u2019t know if it is going to work or not.\u201d\n\nThe research was designed to build an AI that could determine how a patient will respond and whether or not the treatment is likely to work. The model could also help to direct a patient to other treatments.\n\nThe cancer treatment breakthrough according to Thomas Clozel, co-founder and CEO of Owkin is predicated on the success of the federated learning model that is able to gather more data to train the AI than what had been done previously.\n\n\u201cWe want to build federated learning to break competitive and research silos,\u201d Clozel told VentureBeat. \u201cIt\u2019s about human connection and being able to really create this federated network of the best practitioners in the field and researchers being able to work together.\u201d",
            "title": "Federated learning AI model could lead to healthcare breakthrough",
            "keywords": [
                "healthcare",
                "model",
                "able",
                "owkin",
                "breakthrough",
                "federated",
                "data",
                "researchers",
                "lead",
                "learning",
                "patient",
                "ai",
                "substra"
            ],
            "link": "https://venturebeat.com/ai/federated-learning-ai-model-could-lead-to-healthcare-breakthrough/",
            "skip": "false",
            "gpt_score_reason": "\n\n7/10",
            "gpt_relevancy_score": 7,
            "summary": "\n\nOwkin and researchers from four hospitals in France have developed a federated learning model that uses open source software which leverages technologies such as PyTorch and Hyperledger. This breakthrough technology is able to accurately identify novel biomarkers to help make accurate predictions on how breast cancer patients will respond to chemotherapy, leading to improved personalized medical care."
        },
        "15": {
            "text": "We're taking part in Copyright Week, a series of actions and discussions supporting key principles that should guide copyright policy. Every day this week, various groups are taking on different elements of copyright law and policy, and addressing what's at stake and what we need to do to make sure that copyright promotes creativity and innovation.\n\nArtificial Intelligence (AI) grabs headlines with new tools like ChatGPT and DALL-E 2, but it is already here and having major impacts on our lives. Increasingly we see law enforcement, medical care, schools and workplaces all turning to the black box of AI to make life-altering decisions\u2014a trend we should challenge at every turn.\n\nThe vast and often secretive data sets behind this technology, used to train AI with machine learning, come with baggage. Data collected through surveillance and exploitation will reflect systemic biases and be \u201clearned\u201d in the process. In their worst form, the buzzwords of AI and machine learning are used to \"tech wash\" this bias, allowing the powerful to buttress oppressive practices behind the supposed objectivity of code.\n\nIt's time to break open these black boxes. Embracing collaboratively maintained Open Data sets in the development of AI would not only be a boon to transparency and accountability for these tools, but makes it possible for the would-be subjects to create their own innovative and empowering work and research. We need to reclaim this data and harness the power of a democratic and open science to build better tools and a better world.\n\nGarbage in, Gospel out\n\nMachine Learning is a powerful tool, and there are many impressive use-cases: like searching for signs of life on Mars or building synthetic antibodies. But at their core these algorithms are only as \"intelligent\" as the data they're fed. You know the saying: \"garbage in, garbage out.\" Machine Learning ultimately relies on training data to learn how to make good guesses\u2014the logic behind which is typically unknown even to the developers. But even the best guesses shouldn\u2019t be taken as gospel.\n\nThings turn dire when this veiled logic is used to make life-altering decisions. Consider the impact of predictive policing tools, which are built on a foundation of notoriously inaccurate and biased crime data. This AI-enabled search for \"future crimes\" is a perfect example of how this new tool launders biased police data into biased policing\u2014with algorithms putting an emphasis on already over-policed neighborhoods. This self-fulfilling prophecy even gets rolled out to predict criminality by the shape of your face. Then when determining cash bail, another algorithm can set the price using data riddled with the same racist and classist biases.\n\nFortunately, transparency laws let researchers identify and bring attention to these issues. Crime data, warts and all, is often made available to the public. This same transparency is not expected from private actors like your employer, your landlord, or your school.\n\nThe answer isn\u2019t simply to make all this data public. Some AI is trained on legitimately sensitive information, even if publicly available. They are toxic assets sourced by a mix of surveillance and compelled data disclosures. Preparation of this data is itself dubious, often relying on armies of highly exploited workers with no avenues to flag issues with the data or its processing. And despite many \"secret sauce\" claims, anonymizing these large datasets is very difficult and maybe even impossible, and the impacts of a breach would disproportionately impact the people tracked and exploited to produce it.\n\nInstead, embracing collaboratively maintained open data sets would empower data scientists, who are already experts in transparency and privacy issues pertaining to data, to maintain them more ethically. By pooling resources in this way, consensual and transparent data collection would help address these biases, but unlock the creative potential of open science for the future of AI.\n\nAn Open and Empowering Future of AI\n\nAs we see elsewhere in Open Access, this removal of barriers and paywalls helps less-resourced people access and build expertise. The result could be an ecosystem where AI doesn\u2019t just serve the haves over the have-nots, but in which everyone can benefit from the development of these tools.\n\nOpen Source software has long proven the power of pooling resources and collective experimentation. The same holds true of Open Data\u2014making data openly accessible can identify deficits and let people build on one another's work more democratically. Purposefully biasing data (or \"data poisoning\") is possible and this unethical behavior already happens in less transparent systems and is harder to catch. While a move towards using Open Data in AI development would help mitigate bias and phony claims, it\u2019s not a panacea; even harmful and secretive tools can be built with good data.\n\nBut an open system for AI development, from data, to code, to publication, can bring many humanitarian benefits, like in AI\u2019s use in life-saving medical research. The ability to remix and quickly collaborate on medical research can supercharge the research process and uncover missed discoveries in the data. The result? Tools for lifesaving medical diagnosis and treatments for all peoples, mitigating the racial, gender, and other biases in medical research.\n\nOpen Data makes data work for the people. While the expertise and resources needed for machine learning remain a barrier for many, crowd-sourced projects like Open Oversight already empower communities by making information about law enforcement visibility and transparency. Being able to collect, use, and remix data to make their own tools brings AI research from the ivory towers to the streets and breaks down oppressive power imbalances.\n\nOpen Data is not just about making data accessible. It's about embracing the perspectives and creativity of all people to set the groundwork for a more equitable and just society. It's about tearing down exploitative data harvesting and making sure everyone benefits from the future of AI.",
            "title": "Open Data and the AI Black Box",
            "keywords": [
                "machine",
                "tools",
                "medical",
                "black",
                "data",
                "box",
                "future",
                "learning",
                "transparency",
                "research",
                "ai",
                "open"
            ],
            "link": "https://www.eff.org/deeplinks/2023/01/open-data-and-ai-black-box",
            "skip": "false",
            "gpt_score_reason": "\n\n5/10",
            "gpt_relevancy_score": 5,
            "summary": "\n\nThis week, as part of Copyright Week we're talking about harnessing the power of Open Data to work for everyone. To challenge AI and machine learning systems from becoming tools of oppression, we must make data more broadly accessible and democratically controlled. With open access in key areas - like law enforcement data sets, medical research, facial recognition databases \u2013 society can benefit from a transparent set up that unlocks the creative potential of all people for a better world."
        },
        "16": {
            "text": "Geralt/Pixabay\n\nA new study conducted by scientists affiliated with the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany, and the University of Leipzig Medical Center demonstrates that artificial intelligence (AI) machine learning can detect rare types of dementia using medical images.\n\n\u201cDementia syndromes can be difficult to diagnose,\u201d the researchers wrote. \u201cWe aimed at building a classifier for multiple dementia syndromes using magnetic resonance imaging (MRI).\u201d\n\nWorldwide, dementia and average life expectancy are increasing. By 2050, people over the age of 65 will rise to nearly 1.5 billion compared to 524 million in 2010, according to estimates from a report by the World Health Organization and the U.S. National Institute on Aging and U.S. National Institutes of Health. According to the same report, life expectancy at birth is at least 81 years in several countries. Alarmingly, the number of people with dementia is expected to nearly triple worldwide to 152.8 million by 2050 from 27.4 million in 2019, according to The Global Burden of Disease dementia study published last year in The Lancet Public Health.\n\nDementia is a broad term characterized by changes in the brain with common symptoms that may include psychological changes such as anxiety, depression, personality changes, agitation, paranoia, inappropriate behavior, hallucinations, as well as cognitive challenges such as memory loss, confusion, disorientation, and difficulty with thinking, communication, reasoning, problem-solving, planning, organization, coordination, or motor function.\n\nAccording to Stanford Medicine, dementia is due to damage or changes in the brain. Several diseases may cause dementia. Common causes include Alzheimer\u2019s disease, vascular dementia, Parkinson\u2019s disease, Pick\u2019s disease, frontotemporal dementia, and dementia with Lewy bodies. Less common causes of AD include Huntington\u2019s disease, Leukoencephalopathies, Creutzfeldt-Jakob disease, amyotrophic lateral sclerosis (ALS), multiple sclerosis (MS), syphilis, and multiple-system atrophy, a group of degenerative brain diseases per Stanford Medicine.\n\nThe most common cause of progressive dementia is Alzheimer's disease (AD), which accounts for 60 percent to 70 percent of dementia patients, according to the World Health Organization. By 2050 an estimated 16 million people in the U.S. will have Alzheimer\u2019s disease, according to Harvard Medical School. Roughly 5.8 million people in the U.S. are living with Alzheimer\u2019s disease, of which two-thirds are women, according to a report by AARP and the Women\u2019s Alzheimer\u2019s Movement (WAM).\n\nThe neuropsychiatric symptoms associated with Alzheimer\u2019s disease may include depression, social withdrawal, psychosis, wandering, apathy, agitation, distrust in others, disinhibition, and delusions. Short-term memory loss is a common symptom in the early stage of Alzheimer\u2019s disease. In the later stages of the disease, people with Alzheimer's forget how to perform basic daily tasks and are eventually dependent on caregivers for survival.\n\nThe scientists used a supervised machine learning AL algorithm with robust prediction methods called a support vector machine (SVM) to classify the data. The AI was used to classify patient group versus controls (binary classification), as well as a multi-syndrome classifier with all seven diagnostic groups against each other (multiclass classification).\n\n\u201cTo the best of our knowledge, this is one of the first studies assessing computerized methods to differentiate multiple (here seven) dementia syndromes based on atrophy patterns with MRI-derived volumetric data of the brain and SVM,\u201d the researchers reported.\n\nThe study used data from the German Research Consortium of FTLD that included a multicenter cohort using data from 477 subjects consisting of 426 patients and 51 healthy controls. Among the patients, there were 146 with behavioral variant frontotemporal dementia (bvFTD), 72 with Alzheimer\u2019s disease, 58 with progressive nonfluent aphasia (PNFA or nfvPPA\u2013nonfluent/agrammatic variant of PPA), 48 with progressive supranuclear palsy (PSP), 46 with semantic variant primary progressive aphasia (svPPA), 30 with logopenic variant primary progressive aphasia (lvPPA), and 26 with corticobasal syndrome (CBS).\n\nThe researchers hypothesized that the AI binary classifiers and the multi-syndrome classifiers could reach high accuracies in differentiating syndromes. The results were promising for the binary classification, less so for the multi-syndrome classifier. The multi-syndrome model performance was over three times higher than chance level, but only at a 47 percent accuracy, according to the researchers. As a proof-of-principle, the researchers concluded that the multi-syndrome classification has promise but is not translatable to clinical settings at this time.\n\nOn the other hand, the AI machine learning models for binary classification achieved high prediction accuracy that ranged from 71 percent to 95 percent. The study suggests that using AI to differentiate between dementia syndromes and healthy controls is \u201cready for translation to clinical routine if validated in external prospective cohorts in the future.\u201d\n\n\"Results suggest that automated methods applied to MR imaging data can support physicians in diagnosis of dementia syndromes,\u201d the researchers concluded. \u201cIt is particularly relevant for orphan diseases besides frequent syndromes such as Alzheimer\u2019s disease.\u201d\n\nCopyright \u00a9 2023 Cami Rosso All rights reserved.",
            "title": "AI Identifies Rare Forms of Dementia",
            "keywords": [
                "according",
                "disease",
                "multisyndrome",
                "identifies",
                "forms",
                "alzheimers",
                "rare",
                "syndromes",
                "million",
                "data",
                "researchers",
                "dementia",
                "progressive",
                "ai"
            ],
            "link": "https://www.psychologytoday.com/us/blog/the-future-brain/202301/ai-identifies-rare-forms-of-dementia",
            "skip": "false",
            "gpt_score_reason": "\n\n8/10",
            "gpt_relevancy_score": 8,
            "summary": "\n\nA new study conducted by scientists affiliated with the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany, and the University of Leipzig Medical Center has demonstrated that AI machine learning can detect rare types of dementia using medical images. The AI models used achieved high prediction accuracy that ranged from 71-95% when distinguishing between dementia syndromes vs healthy controls. Although a multi-syndrome classifier only achieved 47% accuracy, it provides promising evidence that suggests this method could be used to diagnose dementia in the future."
        },
        "17": {
            "text": "Dell Technologies has expanded its server portfolio, with an additional 13 next-generation Dell PowerEdge servers, designed to accelerate performance and reliability for computing across core data centres, large-scale public clouds and edge locations.\n\nNext-generation rack, tower, and multi-node PowerEdge servers, with 4th Gen Intel Xeon Scalable processors, include Dell software and engineering advancements, such as a new Smart Flow design, to improve energy and cost efficiency.\n\nExpanded Dell APEX capabilities will help organisations take an as-a-Service approach, allowing for more effective IT operations that make the most of compute resources while minimising risk, Dell states.\n\nJeff Boudreau, President and General Manager, Infrastructure Solutions Group, Dell Technologies, says, \u201cCustomers come to Dell for easily managed yet sophisticated and efficient servers with advanced capabilities to power their business-critical workloads.\n\n\u201cOur next-generation Dell PowerEdge servers offer unmatched innovation that raises the bar in power efficiency, performance and reliability while simplifying how customers can implement a zero trust approach for greater security throughout their IT environments.\u201d\n\nNew Dell PowerEdge servers are designed to meet the needs of a range of demanding workloads from artificial intelligence and analytics to large-scale databases.\n\nThe expanded portfolio announced in November 2022, including the PowerEdge XE family of servers with NVIDIA H100 Tensor Core GPUs and the NVIDIA AI Enterprise software suite for a full stack, production AI platform builds on advancements in artificial intelligence and machine learning.\n\nNew servers for cloud service providers\n\nThe introduction of Dell PowerEdge HS5610 and HS5620 servers delivers solutions tailored for cloud service providers managing large-scale, multi-vendor data centres.\n\nAvailable in both 1U and 2U form factors, these new, two-socket servers include cold aisle serviceable configurations and are available with Dell Open Server Manager, an OpenBMC based systems management solution to simplify multi-vendor fleet management.\n\nGreater performance and simpler management\n\nNext-generation PowerEdge servers provide improved performance, including the Dell PowerEdge R760, which delivers up to 2.9x greater AI inferencing on 4th Gen Intel Xeon Scalable processors with Intel Deep Learning Boost and Intel Advanced Matrix Extensions.\n\nThe PowerEdge R760 also offers up to a 20% increase in VDI users and over 50% more SAP Sales & Distribution users on one server, compared to the previous generation. PowerEdge systems may be ordered with NVIDIA Bluefield-2 data processing units to provide additional offload, acceleration and workload isolation capabilities idea for power efficiency for private, hybrid and multi-cloud deployments.\n\nEnhancements to Dell monitoring software and new services make server management easier:\n\nDell CloudIQL: Dell software combines proactive monitoring, machine learning and predictive analytics while offering a comprehensive view of servers wherever they reside. Updates include advancements to server performance forecasting, select maintenance operations and new virtualisation visualisation.\n\n\u00b7 Dell ProDeploy services: The Dell ProDeploy Factory Configuration service delivers PowerEdge servers ready to install and preconfigured with the customer\u2019s preferred operating system, hypervisor software and settings for RAID, BIOS and iDRAC. The Dell ProDeploy Rack Integration service delivers and installs production-ready racked and networked PowerEdge servers, ideal for companies expanding their data centre environments or undergoing an IT modernisation.\n\n\u00b7 Dell iDRAC9: As customers seek increased server automation and intelligence, Dell Remote Access Controller (iDRAC) makes Dell systems easier to deploy and diagnose, equipped with updated features such as Certificate Expiry Notice, Telemetry for Dell Consoles and GPU monitoring.\n\nDesigned for sustainability\n\nDell PowerEdge servers are designed with sustainability in mind, offering customers a 3x performance improvement, compared to 14th Generation PowerEdge servers with Intel Xeon Scalable processors launched in 2017, resulting in less floor space required and more powerful and efficient technology across all next-generation systems.\n\nKey highlights include:\n\nDell Smart Flow design: A new feature within the Dell Smart Cooling suite increases airflow and reduces fan power by up to 52% compared to previous generation servers. The Smart Flow design supports greater server performance with less power required to cool systems for more efficient data centres.\n\nDell OpenManage Enterprise Power Manager 3.0 software: Customers can better manage efficiency and cooling goals, monitor carbon emissions and set power caps up to 82% faster to limit overall energy usage. With the enhanced sustainability target tool, customers can determine overall server use, virtual machine and facility energy consumption, leak detection for liquid cooling systems, and more.\n\nElectronic Product Environmental Assessment Tool (EPEAT): Four next-generation Dell PowerEdge servers will be available with the EPEAT silver designation, and 46 systems will be designated EPEAT bronze. The EPEAT ecolabel is a leading global designation, covering products and services from the technology sector that demonstrate a responsible purchasing decision.\n\nKuba Stolarski, Research Vice President, IDC Enterprise Infrastructure Practice, comments, \u201cToday\u2019s modern data centre requires continuous performance improvements for complex workloads such as AI, ML and VDI.\n\n\u201cAs data centre operators endeavour to keep up with the demand from these resource hungry workloads, they must also prioritise environmental and security goals. With its new Smart Flow design, coupled with enhancements to its power and cooling management tools, Dell offers organisations significant improvements in efficient server operation alongside the raw performance gains in its newest generation of servers.\u201d\n\nReliability and security at the core\n\nNext-generation PowerEdge servers help accelerate zero trust adoption within organisations\u2019 IT environments. The devices constantly verify access, assuming every user and device is a potential threat, according to Dell.\n\nAt the hardware level, silicon-based hardware root of trust, with elements including the Dell Secured Component Verification (SCV), helps verify supply chain security from design to delivery.\n\nAdditionally, multi-factor authentication and integrated iDRAC verifies users before granting access. A secure supply chain also enables customers to advance their zero trust approach. Dell SCV offers cryptographic verification of components, which extends supply chain security to the customer\u2019s site.\n\nDelivering a scalable, modern compute experience Customers looking for OpEx flexibility can consume PowerEdge servers as a subscription through Dell APEX today.\n\nUsing advanced data collection and processor-based measurement by the hour, customers can take a flexible approach to avoid the costs associated with over-provisioning their compute needs.\n\nLater this year, Dell Technologies states the company will expand its Dell APEX portfolio to offer bare metal compute services on-premises, at the edge, or in colocation facilities. Services will be available through a predictable, monthly subscription and easily configured through the APEX Console, enabling customers to meet their workload and IT operational needs with scalable and secure compute resources.\n\nLisa Spelman, Corporate Vice President and General Manager, Intel Xeon Products, says, \u201c4th Gen Intel Xeon Scalable processors have the most built-in accelerators of any CPU on the market to help maximise performance efficiency for real world applications, especially those powered by AI.\n\n\u201cWith the latest generation of Dell PowerEdge servers, Intel and Dell continue our strong collaboration in delivering innovations that create real business value, while incorporating leading scalability and security that customers require.\u201d",
            "title": "Dell Technologies expands server portfolio with 13 new additions",
            "keywords": [
                "power",
                "additions",
                "poweredge",
                "portfolio",
                "performance",
                "expands",
                "intel",
                "dell",
                "systems",
                "data",
                "technologies",
                "customers",
                "servers",
                "13",
                "server"
            ],
            "link": "https://itbrief.com.au/story/dell-technologies-expands-server-portfolio-with-13-new-additions",
            "skip": "false",
            "gpt_score_reason": "\n\n7/10",
            "gpt_relevancy_score": 7,
            "summary": "\n\nDell Technologies has released 13 new Dell PowerEdge servers to meet the needs of a range of demanding workloads. With updated monitoring software, sustainability features and cold aisle serviceable configurations, these servers provide improved energy efficiency for data centers and cloud services as well as greater performance with up to 2.9x faster AI inferencing. In addition, the servers implement zero trust security methods such as multi-factor authentication and cryptographic verification to ensure reliability and safety in IT environments."
        },
        "18": {
            "text": "The best way to ensure electronic health record systems can share data interoperably is for industry to adopt the Fast Healthcare Interoperability Resources standard, or FHIR, say federal officials.\n\nWhile the deployment of a single, consolidated electronic health record, called the Military Health System GENESIS, has the Department of Defense on a path to interoperability, the FHIR data standard that serves as its backbone isn\u2019t congressionally mandated.\n\nFHIR application programming interfaces streamline health information exchange by standardizing data and eliminating the need for sharing agreements, which is why the government required certified EHR vendors to make the APIs available to customers by the end of 2022. Still, unlike credit card companies, which have accepted data standards industrywide, the healthcare industry hasn\u2019t fully rallied behind FHIR.\n\n\u201cWe probably need to get the Chief Data Officers Council to make sure that it covers all the agencies. Congressional mandate, law is another tool,\u201d said Ken Johns, CTO for the Defense Healthcare Management Systems Program Executive Office, at the AFCEA Bethesda Health IT Summit 2023 on Tuesday. \u201cProbably what will be most successful is for industry to agree to standardize.\u201d\n\nClick the banner below to learn more about healthcare IT by becoming an Insider.",
            "title": "Federal Officials Urge Healthcare Industry\u2019s Adoption of FHIR for Interoperability",
            "keywords": [
                "federal",
                "need",
                "industrys",
                "healthcare",
                "officials",
                "probably",
                "fhir",
                "adoption",
                "urge",
                "systems",
                "data",
                "record",
                "health",
                "standard",
                "interoperability",
                "industry"
            ],
            "link": "https://fedtechmagazine.com/article/2023/01/federal-officials-urge-healthcare-industrys-adoption-fhir-interoperability",
            "skip": "false",
            "gpt_score_reason": "\n\n5/10",
            "gpt_relevancy_score": 5,
            "summary": "\nFederal officials believe adoption of the Fast Healthcare Interoperability Resources (FHIR) standard is essential for effective sharing of data between electronic health record systems. The government requires EHR vendors to make the APIs available by 2022, and while FHIR has been accepted in some parts of the industry, it hasn't been fully embraced. To promote greater use and acceptance of FHIR across all agencies, Ken Johns, CTO for the Defense Healthcare Management Systems Program Executive Office suggests that an industry agreement to standardize would be most successful."
        },
        "19": {
            "text": "The fourth quarterly Naval Artificial Intelligence (AI) Summit took place December 5-9 at the Naval Postgraduate School (NPS). More than 160 participants from the eight Navy AI Task Forces, Warfare and Warfighting Development Centers, the fleet, Fleet Marine Force and DOD engaged in-person and virtually at the summit co-hosted by the U.S. Navy Chief AI Officer (NCAIO) Brett Vaughan and NPS\u2019 Naval Warfare Studies Institute.\n\n\n\nDecision Advantage is one of six force design elements and will be enabled by AI as stated in the Chief of Naval Operations\u2019 NAVPLAN 2022. These AI Summits are held to tighten the connections between Naval AI enablers, practitioners and users, as well as grow the Naval AI community into a platform for accelerated AI deployment supporting the Navy\u2019s highest priorities.\n\n\n\n\u201cWe focused attendees time on planning Naval AI training and education, AI/machine learning (ML) operations deployment pipelines, and AI Fusion, a concept for the operational deployment of AI in service to distributed maritime operations and the hybrid fleet,\u201d said Vaughn. \u201cNPS has been instrumental as a central connector and helping us develop a blueprint for a Naval AI organization and governance structure.\u201d\n\n\n\nU.S. Marine Corps Maj. Jack Long, PhD, a Marine Reservist at the Office of Naval Research and Deputy Navy Chief AI Officer, introduced summit attendees to the current state of AI in the Navy and began the discussion of future trends to shape impactful Naval AI development and deployment. The Navy and its AI Task Forces were joined at the summit by partners from the U.S. Army, U.S. Marine Corps, U.S. Coast Guard, the Defense Intelligence Agency, the Joint Staff and the Chief Digital and AI Office (CDAO).\n\n\n\nAI is a tool that becomes ever more applicable as the world becomes more wired, generates more data, and increasingly taps advanced processing power.\n\n\n\n\u201cWe think AI is widely applicable to warfare, so we want to make sure the Naval services and the DoD in general are able to efficiently harness this emergent and disruptive tool. It's important that we are ready to use it for our own purposes, whether in warfighting or corporate functions, and be ready to have it used against us,\u201d said Long.\n\n\n\nWith discussions held at varied classification levels, a significant part of the summit focused on skills that the Navy and Marine Corps will need to channel this capability. Past innovations like submarines and aviation have reshaped naval warfare, with each invention requiring new doctrine, equipment, and personnel to support it. As the required skills become more specialized, those personnel have required new career paths to allow them to become masters at their crafts.\n\n\n\nThe Navy must learn to operate at the speed of AI. As expertise becomes internal, the Navy will need a workforce capable of coding, building models, and harnessing AI in a similar fashion to how Sailors once had to know how to use a sextant. AI application would become part of basic seamanship. Alternatively, the Navy could rely on industry and contractors to bend AI to the Navy\u2019s purposes in much the same way as the service has them build ships.\n\n\n\nThis summit started the conversation on what the services will need to have about how they define their core competency and what they choose to not do.\n\n\n\n\u201cRight now, there's a small cadre of people in the Navy that have thought about this extensively,\u201d said Long. \u201cBefore the CNO or CMC sign off on strategic AI decisions, a much wider group must be engaged. Ultimately this is a conversation about the future identity of the Services in an age of digital warfare.\u201d\n\n\n\nDuring the week-long summit, attendees connected, received informational briefs, and took part in several practical workshops on topics such as Naval AI training and education, practical AI/ML deployment pipeline architecture and operational AI employment at the forward edge of naval operations.\n\n\n\nSome of these conversations will inspire projects and partnerships that will be realized through a Naval Innovation Exchange (NIX) led by Dr. Mathias Kolsch at NPS. Kolsch is working to tie the summit, courses and research together.\n\n\n\n\u201cI am figuring out the ideal workforce composition for AI/ML involved projects, and the AI Summit also informs me how to do that,\u201d Kolsch explained. \u201cI meet and talk to these people. I see where there are successful projects and begin asking questions. How many are listed here for this? What do they know? Can they code? \u2026 What are the skills needed for this project? So, this summit informs the NIX.\u201d\n\n\n\nCooperative planning for the next Naval AI Summit in March 2023 has already begun, and Kolsch plans to offer a version of the popular \u201cAI for Leadership\u201d course specifically designed to support active general officers, flag officers, and Senior Executive Service civilians during the same week.\n\n\n\nNPS is currently a center of gravity for advanced AI training and education within the Naval Education Enterprise. If the NCAIO and Long have their way, that role will grow in the near term. At the crux of research and operational experience, NPS is an ideal center for AI advanced education and implementation.\n\n\n\n\u201cThe vision of accelerated Naval AI training and education, anchored at NPS, closely resembles the role of Top Gun for Naval Aviators. NPS as a center for advanced AI training in much the same way pilots go to flight school and then later in their career go back for advanced training to keep current and learn how to employ the craft at the pace of industry and adversaries,\u201d said Long. \u201cThat's the core role that we see NPS playing in the coming years \u2013 the central hub for the diffusion of AI knowledge across the Navy and Marine Corps.\u201d\n\n\n\nSince inception of the program, the AI Summit continues to grow and evolve. What began as an ambitious effort of less than a dozen practitioners has grown by orders of magnitude. And it\u2019s a number that will surely grow as the Navy and Marine Corps further embrace the widespread applications of AI, and the necessity to evolve and innovate at great speed and scale. NPS will continue to play a leading role in AI education, research and innovation with the next AI Summit scheduled for March 2023.\n\nNEWS INFO Date Taken: 01.19.2023 Date Posted: 01.19.2023 17:36 Story ID: 436975 Location: MONTEREY, CA, US Web Views: 27 Downloads: 0 PUBLIC DOMAIN This work, Artificial Intelligence Summit at NPS Accelerates Critical Capabilities, must comply with the restrictions shown on https://www.dvidshub.net/about/copyright.",
            "title": "Artificial Intelligence Summit at NPS Accelerates Critical Capabilities",
            "keywords": [
                "intelligence",
                "deployment",
                "capabilities",
                "summit",
                "critical",
                "nps",
                "naval",
                "navy",
                "education",
                "artificial",
                "advanced",
                "accelerates",
                "training",
                "ai",
                "marine"
            ],
            "link": "https://www.dvidshub.net/news/436975/artificial-intelligence-summit-nps-accelerates-critical-capabilities",
            "skip": "true",
            "duplicates": [
                "7",
                "19"
            ],
            "gpt_score_reason": "",
            "gpt_relevancy_score": 0
        }
    }
}